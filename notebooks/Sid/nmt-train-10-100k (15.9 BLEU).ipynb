{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/ymoslem/OpenNMT-Tutorial/blob/main/2-NMT-Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"# Install OpenNMT-py 3.x\n!pip3 install OpenNMT-py==3.4","metadata":{"id":"vSUyCs23M_H2","execution":{"iopub.status.busy":"2023-11-08T06:39:13.615568Z","iopub.execute_input":"2023-11-08T06:39:13.615857Z","iopub.status.idle":"2023-11-08T06:39:30.410405Z","shell.execute_reply.started":"2023-11-08T06:39:13.615831Z","shell.execute_reply":"2023-11-08T06:39:30.409455Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting OpenNMT-py==3.4\n  Downloading OpenNMT_py-3.4-py3-none-any.whl (250 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch<2.1,>=1.13 in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (2.0.0)\nCollecting configargparse (from OpenNMT-py==3.4)\n  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\nCollecting ctranslate2<4,>=3.2 (from OpenNMT-py==3.4)\n  Downloading ctranslate2-3.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (2.12.3)\nRequirement already satisfied: flask in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (2.3.3)\nCollecting waitress (from OpenNMT-py==3.4)\n  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py==3.4)\n  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (6.0)\nCollecting sacrebleu (from OpenNMT-py==3.4)\n  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: rapidfuzz in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (3.2.0)\nCollecting pyahocorasick (from OpenNMT-py==3.4)\n  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py==3.4)\n  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (3.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ctranslate2<4,>=3.2->OpenNMT-py==3.4) (1.23.5)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (68.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (2.3.7)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (0.40.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (3.1.2)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext-wheel->OpenNMT-py==3.4) (2.11.1)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->OpenNMT-py==3.4) (2.1.2)\nRequirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from flask->OpenNMT-py==3.4) (8.1.7)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask->OpenNMT-py==3.4) (1.6.2)\nCollecting portalocker (from sacrebleu->OpenNMT-py==3.4)\n  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu->OpenNMT-py==3.4) (2023.6.3)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->OpenNMT-py==3.4) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->OpenNMT-py==3.4) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->OpenNMT-py==3.4) (4.9.3)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (0.10.1)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (4.66.1)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (1.10.9)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (3.3.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (4.9)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py==3.4) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<2.1,>=1.13->OpenNMT-py==3.4) (2.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy->OpenNMT-py==3.4) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py==3.4) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py==3.4) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py==3.4) (2023.7.22)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py==3.4) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py==3.4) (0.1.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<2.1,>=1.13->OpenNMT-py==3.4) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py==3.4) (3.2.2)\nInstalling collected packages: waitress, pyonmttok, pyahocorasick, portalocker, fasttext-wheel, ctranslate2, configargparse, sacrebleu, OpenNMT-py\nSuccessfully installed OpenNMT-py-3.4 configargparse-1.7 ctranslate2-3.20.0 fasttext-wheel-0.9.2 portalocker-2.8.2 pyahocorasick-2.0.0 pyonmttok-1.37.1 sacrebleu-2.3.2 waitress-2.1.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare Your Datasets\nPlease make sure you have completed the [first exercise](https://colab.research.google.com/drive/1rsFPnAQu9-_A6e2Aw9JYK3C8mXx9djsF?usp=sharing).","metadata":{"id":"vhgIdJn-cLqu"}},{"cell_type":"code","source":"# Open the folder where you saved your prepapred datasets from the first exercise\n# You might need to mount your Google Drive first\n# %cd /content/drive/MyDrive/nmt/\n# !ls","metadata":{"id":"dWVOWYedzZ_G","execution":{"iopub.status.busy":"2023-11-08T06:39:30.412266Z","iopub.execute_input":"2023-11-08T06:39:30.412556Z","iopub.status.idle":"2023-11-08T06:39:30.416766Z","shell.execute_reply.started":"2023-11-08T06:39:30.412527Z","shell.execute_reply":"2023-11-08T06:39:30.415916Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Create the Training Configuration File\n\nThe following config file matches most of the recommended values for the Transformer model [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). As the current dataset is small, we reduced the following values: \n* `train_steps` - for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more! Enabling the option `early_stopping` can help stop the training when there is no considerable improvement.\n* `valid_steps` - 10000 can be good if the value `train_steps` is big enough. \n* `warmup_steps` - obviously, its value must be less than `train_steps`. Try 4000 and 8000 values.\n\nRefer to [OpenNMT-py training parameters](https://opennmt.net/OpenNMT-py/options/train.html) for more details. If you are interested in further explanation of the Transformer model, you can check this article, [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).","metadata":{"id":"MPlmhd426B7l"}},{"cell_type":"code","source":"# Create the YAML configuration file\n# On a regular machine, you can create it manually or with nano\n# Note here we are using some smaller values because the dataset is small\n# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n\nconfig = '''# config.yaml\n\n\n## Where the samples will be written\nsave_data: run\n\n# Training files\ndata:\n    corpus_1:\n        path_src: /kaggle/input/nmt-data-subword/split_train2023_10kto100k.vi.subword\n        path_tgt: /kaggle/input/nmt-data-subword/split_train2023_10kto100k.lo.subword\n        transforms: [filtertoolong]\n    valid:\n        path_src: /kaggle/input/nmt-data-subword/split_val2023_10kto100k.vi.subword\n        path_tgt: /kaggle/input/nmt-data-subword/split_val2023_10kto100k.lo.subword\n        transforms: [filtertoolong]\n\n# Vocabulary files, generated by onmt_build_vocab\nsrc_vocab: run/source.vocab\ntgt_vocab: run/target.vocab\n\n# Vocabulary size - should be the same as in sentence piece\nsrc_vocab_size: 50000\ntgt_vocab_size: 50000\n\n# Filter out source/target longer than n if [filtertoolong] enabled\nsrc_seq_length: 150\nsrc_seq_length: 150\n\n# Tokenization options\nsrc_subword_model: /kaggle/input/nmt-data-subword/source.model\ntgt_subword_model: /kaggle/input/nmt-data-subword/target.model\n\n# Where to save the log file and the output models/checkpoints\nlog_file: train.log\nsave_model: models/model.vilo\n\n# Stop training if it does not imporve after n validations\nearly_stopping: 4\n\n# Default: 5000 - Save a model checkpoint for each n\nsave_checkpoint_steps: 5000\n\n# To save space, limit checkpoints to last n\n# keep_checkpoint: 3\n\nseed: 3435\n\n# Default: 100000 - Train the model to max n steps \n# Increase to 200000 or more for large datasets\n# For fine-tuning, add up the required steps to the original steps\ntrain_steps: 100000\n\n# Default: 10000 - Run validation after n steps\nvalid_steps: 1000\n\n# Default: 4000 - for large datasets, try up to 8000\nwarmup_steps: 1000\nreport_every: 100\n\n# Number of GPUs, and IDs of GPUs\nworld_size: 1\ngpu_ranks: [0]\n\n# Batching\nbucket_size: 262144\nnum_workers: 0  # Default: 2, set to 0 when RAM out of memory\nbatch_type: \"tokens\"\nbatch_size: 4096   # Tokens per batch, change when CUDA out of memory\nvalid_batch_size: 2048\nmax_generator_batches: 2\naccum_count: [4]\naccum_steps: [0]\n\n# Optimization\nmodel_dtype: \"fp16\"\noptim: \"adam\"\nlearning_rate: 2\n# warmup_steps: 8000\ndecay_method: \"noam\"\nadam_beta2: 0.998\nmax_grad_norm: 0\nlabel_smoothing: 0.1\nparam_init: 0\nparam_init_glorot: true\nnormalization: \"tokens\"\n\n# Model\nencoder_type: transformer\ndecoder_type: transformer\nposition_encoding: true\nenc_layers: 6\ndec_layers: 6\nheads: 8\nhidden_size: 512\nword_vec_size: 512\ntransformer_ff: 2048\ndropout_steps: [0]\ndropout: [0.1]\nattention_dropout: [0.1]\n'''\n\nwith open(\"config.yaml\", \"w+\") as config_yaml:\n  config_yaml.write(config)","metadata":{"id":"qbW7Xek6UDlY","execution":{"iopub.status.busy":"2023-11-08T06:39:30.418047Z","iopub.execute_input":"2023-11-08T06:39:30.418322Z","iopub.status.idle":"2023-11-08T06:39:30.429404Z","shell.execute_reply.started":"2023-11-08T06:39:30.418300Z","shell.execute_reply":"2023-11-08T06:39:30.428712Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# [Optional] Check the content of the configuration file\n!cat config.yaml","metadata":{"id":"vsL4zycvLMUx","execution":{"iopub.status.busy":"2023-11-08T06:39:30.431714Z","iopub.execute_input":"2023-11-08T06:39:30.431984Z","iopub.status.idle":"2023-11-08T06:39:31.407619Z","shell.execute_reply.started":"2023-11-08T06:39:30.431962Z","shell.execute_reply":"2023-11-08T06:39:31.406493Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"# config.yaml\n\n\n## Where the samples will be written\nsave_data: run\n\n# Training files\ndata:\n    corpus_1:\n        path_src: /kaggle/input/nmt-data-subword/split_train2023_10kto100k.vi.subword\n        path_tgt: /kaggle/input/nmt-data-subword/split_train2023_10kto100k.lo.subword\n        transforms: [filtertoolong]\n    valid:\n        path_src: /kaggle/input/nmt-data-subword/split_val2023_10kto100k.vi.subword\n        path_tgt: /kaggle/input/nmt-data-subword/split_val2023_10kto100k.lo.subword\n        transforms: [filtertoolong]\n\n# Vocabulary files, generated by onmt_build_vocab\nsrc_vocab: run/source.vocab\ntgt_vocab: run/target.vocab\n\n# Vocabulary size - should be the same as in sentence piece\nsrc_vocab_size: 50000\ntgt_vocab_size: 50000\n\n# Filter out source/target longer than n if [filtertoolong] enabled\nsrc_seq_length: 150\nsrc_seq_length: 150\n\n# Tokenization options\nsrc_subword_model: /kaggle/input/nmt-data-subword/source.model\ntgt_subword_model: /kaggle/input/nmt-data-subword/target.model\n\n# Where to save the log file and the output models/checkpoints\nlog_file: train.log\nsave_model: models/model.fren\n\n# Stop training if it does not imporve after n validations\nearly_stopping: 4\n\n# Default: 5000 - Save a model checkpoint for each n\nsave_checkpoint_steps: 1000\n\n# To save space, limit checkpoints to last n\n# keep_checkpoint: 3\n\nseed: 3435\n\n# Default: 100000 - Train the model to max n steps \n# Increase to 200000 or more for large datasets\n# For fine-tuning, add up the required steps to the original steps\ntrain_steps: 3000\n\n# Default: 10000 - Run validation after n steps\nvalid_steps: 1000\n\n# Default: 4000 - for large datasets, try up to 8000\nwarmup_steps: 1000\nreport_every: 100\n\n# Number of GPUs, and IDs of GPUs\nworld_size: 1\ngpu_ranks: [0]\n\n# Batching\nbucket_size: 262144\nnum_workers: 0  # Default: 2, set to 0 when RAM out of memory\nbatch_type: \"tokens\"\nbatch_size: 4096   # Tokens per batch, change when CUDA out of memory\nvalid_batch_size: 2048\nmax_generator_batches: 2\naccum_count: [4]\naccum_steps: [0]\n\n# Optimization\nmodel_dtype: \"fp16\"\noptim: \"adam\"\nlearning_rate: 2\n# warmup_steps: 8000\ndecay_method: \"noam\"\nadam_beta2: 0.998\nmax_grad_norm: 0\nlabel_smoothing: 0.1\nparam_init: 0\nparam_init_glorot: true\nnormalization: \"tokens\"\n\n# Model\nencoder_type: transformer\ndecoder_type: transformer\nposition_encoding: true\nenc_layers: 6\ndec_layers: 6\nheads: 8\nhidden_size: 512\nword_vec_size: 512\ntransformer_ff: 2048\ndropout_steps: [0]\ndropout: [0.1]\nattention_dropout: [0.1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Build Vocabulary\n\nFor large datasets, it is not feasable to use all words/tokens found in the corpus. Instead, a specific set of vocabulary is extracted from the training dataset, usually betweeen 32k and 100k words. This is the main purpose of the vocabulary building step.","metadata":{"id":"F0bcqYkEXhRY"}},{"cell_type":"code","source":"# Find the number of CPUs/cores on the machine\n!nproc --all","metadata":{"id":"AuwltKp_VhnQ","outputId":"4d9d5e5e-df7b-474b-b281-369424c47603","execution":{"iopub.status.busy":"2023-11-08T06:39:31.409067Z","iopub.execute_input":"2023-11-08T06:39:31.409390Z","iopub.status.idle":"2023-11-08T06:39:32.372769Z","shell.execute_reply.started":"2023-11-08T06:39:31.409359Z","shell.execute_reply":"2023-11-08T06:39:32.371669Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}]},{"cell_type":"code","source":"# Build Vocabulary\n\n# -config: path to your config.yaml file\n# -n_sample: use -1 to build vocabulary on all the segment in the training dataset\n# -num_threads: change it to match the number of CPUs to run it faster\n\n!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 4","metadata":{"id":"P2GV1PgyUsJr","outputId":"e1749505-d1f6-41cd-9420-1876db27e405","execution":{"iopub.status.busy":"2023-11-08T06:39:32.374310Z","iopub.execute_input":"2023-11-08T06:39:32.374606Z","iopub.status.idle":"2023-11-08T06:39:53.617872Z","shell.execute_reply.started":"2023-11-08T06:39:32.374577Z","shell.execute_reply":"2023-11-08T06:39:53.616757Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-11-08 06:39:48,531 INFO] Counter vocab from -1 samples.\n[2023-11-08 06:39:48,531 INFO] n_sample=-1: Build vocab on full datasets.\n[2023-11-08 06:39:50,840 INFO] * Transform statistics for corpus_1(25.00%):\n\t\t\t* FilterTooLongStats(filtered=409)\n\n[2023-11-08 06:39:50,844 INFO] * Transform statistics for corpus_1(25.00%):\n\t\t\t* FilterTooLongStats(filtered=440)\n\n[2023-11-08 06:39:50,846 INFO] * Transform statistics for corpus_1(25.00%):\n\t\t\t* FilterTooLongStats(filtered=461)\n\n[2023-11-08 06:39:50,848 INFO] * Transform statistics for corpus_1(25.00%):\n\t\t\t* FilterTooLongStats(filtered=384)\n\n[2023-11-08 06:39:50,989 INFO] Counters src: 12288\n[2023-11-08 06:39:50,990 INFO] Counters tgt: 27900\n","output_type":"stream"}]},{"cell_type":"markdown","source":"From the **Runtime menu** > **Change runtime type**, make sure that the \"**Hardware accelerator**\" is \"**GPU**\".\n","metadata":{"id":"ncWyNtxiO_Ov"}},{"cell_type":"code","source":"# Check if the GPU is active\n!nvidia-smi -L","metadata":{"id":"TMMPeS-pSV8I","outputId":"ea51133a-beaa-4642-e8ba-7bd9159ada68","execution":{"iopub.status.busy":"2023-11-08T06:39:53.619380Z","iopub.execute_input":"2023-11-08T06:39:53.619714Z","iopub.status.idle":"2023-11-08T06:39:54.590174Z","shell.execute_reply.started":"2023-11-08T06:39:53.619683Z","shell.execute_reply":"2023-11-08T06:39:54.588937Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-ce52e806-e7e0-68d5-bbb2-24ed59359682)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check if the GPU is visable to PyTorch\n\nimport torch\n\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0))\n\ngpu_memory = torch.cuda.mem_get_info(0)\nprint(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)","metadata":{"id":"_3rVQhd4NXNG","outputId":"181eb6a3-cc09-45b6-de4e-b1e88e45f97b","execution":{"iopub.status.busy":"2023-11-08T06:39:54.591966Z","iopub.execute_input":"2023-11-08T06:39:54.592867Z","iopub.status.idle":"2023-11-08T06:39:58.999352Z","shell.execute_reply.started":"2023-11-08T06:39:54.592824Z","shell.execute_reply":"2023-11-08T06:39:58.998207Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"True\nTesla P100-PCIE-16GB\nFree GPU memory: 15665.75 out of: 16280.875\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training\n\nNow, start training your NMT model! 🎉 🎉 🎉","metadata":{"id":"8aCxETSnXcL-"}},{"cell_type":"code","source":"!rm -rf /kaggle/working/nmt/models/","metadata":{"id":"HZd1o1kIb6Nv","execution":{"iopub.status.busy":"2023-11-08T06:39:59.000793Z","iopub.execute_input":"2023-11-08T06:39:59.001300Z","iopub.status.idle":"2023-11-08T06:40:00.009639Z","shell.execute_reply.started":"2023-11-08T06:39:59.001272Z","shell.execute_reply":"2023-11-08T06:40:00.008358Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Train the NMT model\n!onmt_train -config config.yaml","metadata":{"id":"prJCKA2CP-dl","execution":{"iopub.status.busy":"2023-11-08T06:40:00.013228Z","iopub.execute_input":"2023-11-08T06:40:00.013540Z","iopub.status.idle":"2023-11-08T06:40:12.257841Z","shell.execute_reply.started":"2023-11-08T06:40:00.013511Z","shell.execute_reply":"2023-11-08T06:40:12.256635Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-11-08 06:40:08,149 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n[2023-11-08 06:40:08,150 INFO] Parsed 2 corpora from -data.\n[2023-11-08 06:40:08,150 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n[2023-11-08 06:40:08,345 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁', ',', '0', '.', '1', '2']\n[2023-11-08 06:40:08,345 INFO] The decoder start token is: <s>\n[2023-11-08 06:40:08,345 INFO] Building model...\n[2023-11-08 06:40:09,407 INFO] Switching model to float32 for amp/apex_amp\n[2023-11-08 06:40:09,407 INFO] Non quantized layer compute is fp16\n^C\n","output_type":"stream"}]},{"cell_type":"code","source":"# For error debugging try:\n# !dmesg -T","metadata":{"id":"XUYAvE8ffK2k","execution":{"iopub.status.busy":"2023-11-08T06:40:12.259232Z","iopub.execute_input":"2023-11-08T06:40:12.259561Z","iopub.status.idle":"2023-11-08T06:40:12.264486Z","shell.execute_reply.started":"2023-11-08T06:40:12.259532Z","shell.execute_reply":"2023-11-08T06:40:12.263263Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Translation\n\nTranslation Options:\n* `-model` - specify the last model checkpoint name; try testing the quality of multiple checkpoints\n* `-src` - the subworded test dataset, source file\n* `-output` - give any file name to the new translation output file\n* `-gpu` - GPU ID, usually 0 if you have one GPU. Otherwise, it will translate on CPU, which would be slower.\n* `-min_length` - [optional] to avoid empty translations\n* `-verbose` - [optional] if you want to print translations\n\nRefer to [OpenNMT-py translation options](https://opennmt.net/OpenNMT-py/options/translate.html) for more details.","metadata":{"id":"eShpS01j-Jcp"}},{"cell_type":"code","source":"!git clone https://github.com/ymoslem/MT-Preparation.git","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:40:54.928415Z","iopub.execute_input":"2023-11-08T06:40:54.928732Z","iopub.status.idle":"2023-11-08T06:40:55.898019Z","shell.execute_reply.started":"2023-11-08T06:40:54.928694Z","shell.execute_reply":"2023-11-08T06:40:55.896723Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"fatal: destination path 'MT-Preparation' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Translate the \"subworded\" source file of the test dataset\n# Change the model name, if needed.\n!onmt_translate -model models/model.vilo_step_100000.pt -src /kaggle/input/nmt-data-subword/split_val2023_10kto100k.vi.subword -output UN.lo.translated -gpu 0 -min_length 1","metadata":{"id":"MbQEGTj4TybH","outputId":"b2181f89-47a1-46d3-888c-9facf296bf61","execution":{"iopub.status.busy":"2023-11-08T06:41:08.285316Z","iopub.execute_input":"2023-11-08T06:41:08.286510Z","iopub.status.idle":"2023-11-08T06:49:23.758537Z","shell.execute_reply.started":"2023-11-08T06:41:08.286463Z","shell.execute_reply":"2023-11-08T06:49:23.757462Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-11-08 06:41:15,967 INFO] Loading checkpoint from /kaggle/input/result-model-nmt/model.fren_step_3000.pt\n[2023-11-08 06:41:23,487 INFO] Loading data into the model\n[2023-11-08 06:49:22,274 INFO] PRED SCORE: -0.4574, PRED PPL: 1.58 NB SENTENCES: 12000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the first 5 lines of the translation file\n!head -n 5 UN.lo.translated","metadata":{"id":"XHYihrgfIrIO","outputId":"980d6dad-3467-4157-a398-8fe05509cb54","execution":{"iopub.status.busy":"2023-11-08T06:50:04.095992Z","iopub.execute_input":"2023-11-08T06:50:04.096411Z","iopub.status.idle":"2023-11-08T06:50:05.053065Z","shell.execute_reply.started":"2023-11-08T06:50:04.096373Z","shell.execute_reply":"2023-11-08T06:50:05.051951Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"▁ພ້ອມກັນ ນັ້ນ , ▁ ກະຊວງກະສິກໍາ ▁ແລະ ▁ພັດທະນາຊົນນະບົດ ▁ຫວຽດນາມ ▁ກໍ່ ເພີ່ມທະວີ ວຽກງານ ຈັດຕັ້ງ ການຮັບ ທາ ບ , ▁ພິເສດແມ່ນ ▁ພາຍ ▁ 2 ▁ປະເທດ , ▁ພິເສດແມ່ນ ພະຍຸ , ▁ ປ້ອງກັນ , ▁ຕ້ານ ບັນດາ ຂໍ້ກໍານົດ , ▁ຕ້ານ ເຊື້ອພະຍາດ ຕິດຕໍ່ , ▁ພິເສດແມ່ນ ວິສາຫະກິດ ...\n▁ຈາກ ບໍລິສັດ ແຟັກ : ▁+ ▁ 8 6 - 5 2 3 .\n▁ຂ້າພະ ▁ເຈົ້າ ອະທິຖານ ▁ວ່າ ▁ໃນ ▁ໂລກ ▁ນີ້ ▁ຈະ ▁ບັນລຸ ▁ຄວາມ ▁ສໍາ ▁ເລັດ , ▁ເຮົາ ▁ຈະ ▁ໄດ້ ▁ຮັບ ▁ຄວາມ ▁ຖ່ອມ ▁ຕົນ ▁ເພື່ອ ▁ຈະ ▁ໄດ້ ▁ຮັບ .\n▁ໃນຄວາມເປັນຈິງ , ▁ສັງຄົມ ໄດ້ ຂ້າຕົວ ຕາຍ .\n▁ 1 2 ▁ບັດ ▁ນີ້ ▁ຫລາຍ ▁ຄົນ ▁ບໍ່ ▁ໄດ້ ▁ຕາຍ ▁ໄປ , ▁ແຕ່ ▁ລູກ ▁ກັບ ▁ເມຍ ▁ຂອງ ▁ຕົນ ▁ອີກ , ▁ແຕ່ ▁ເຂົາ ▁ໄດ້ ▁ຕາຍ ▁ໄປ ▁ດ້ວຍ ▁ຄວາມ ▁ຕາຍ .\n","output_type":"stream"}]},{"cell_type":"code","source":"# If needed install/update sentencepiece\n!pip3 install --upgrade -q sentencepiece\n\n# Desubword the translation file\n!python3 MT-Preparation/subwording/3-desubword.py /kaggle/input/nmt-data-subword/target.model UN.lo.translated","metadata":{"id":"zRsJm6UET2C_","outputId":"f9a410d7-e753-4c43-e5cb-82c62ed00a53","execution":{"iopub.status.busy":"2023-11-08T06:51:30.703811Z","iopub.execute_input":"2023-11-08T06:51:30.704150Z","iopub.status.idle":"2023-11-08T06:51:43.828721Z","shell.execute_reply.started":"2023-11-08T06:51:30.704104Z","shell.execute_reply":"2023-11-08T06:51:43.827628Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Done desubwording! Output: UN.lo.translated.desubword\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the first 5 lines of the desubworded translation file\n!head -n 5 UN.lo.translated.desubword","metadata":{"id":"ai4RhhGaKBp1","outputId":"a833acd0-99d0-48ed-ad63-dac8f91ff4da","execution":{"iopub.status.busy":"2023-11-08T06:51:49.444459Z","iopub.execute_input":"2023-11-08T06:51:49.445100Z","iopub.status.idle":"2023-11-08T06:51:50.403578Z","shell.execute_reply.started":"2023-11-08T06:51:49.445064Z","shell.execute_reply":"2023-11-08T06:51:50.402575Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"ພ້ອມກັນນັ້ນ, ກະຊວງກະສິກໍາ ແລະ ພັດທະນາຊົນນະບົດ ຫວຽດນາມ ກໍ່ເພີ່ມທະວີວຽກງານຈັດຕັ້ງການຮັບທາບ, ພິເສດແມ່ນ ພາຍ 2 ປະເທດ, ພິເສດແມ່ນພະຍຸ, ປ້ອງກັນ, ຕ້ານບັນດາຂໍ້ກໍານົດ, ຕ້ານເຊື້ອພະຍາດຕິດຕໍ່, ພິເສດແມ່ນວິສາຫະກິດ...\nຈາກບໍລິສັດແຟັກ: + 86-523.\nຂ້າພະ ເຈົ້າອະທິຖານ ວ່າ ໃນ ໂລກ ນີ້ ຈະ ບັນລຸ ຄວາມ ສໍາ ເລັດ, ເຮົາ ຈະ ໄດ້ ຮັບ ຄວາມ ຖ່ອມ ຕົນ ເພື່ອ ຈະ ໄດ້ ຮັບ.\nໃນຄວາມເປັນຈິງ, ສັງຄົມໄດ້ຂ້າຕົວຕາຍ.\n12 ບັດ ນີ້ ຫລາຍ ຄົນ ບໍ່ ໄດ້ ຕາຍ ໄປ, ແຕ່ ລູກ ກັບ ເມຍ ຂອງ ຕົນ ອີກ, ແຕ່ ເຂົາ ໄດ້ ຕາຍ ໄປ ດ້ວຍ ຄວາມ ຕາຍ.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Desubword the target file (reference) of the test dataset\n# Note: You might as well have split files *before* subwording during dataset preperation, \n# but sometimes datasets have tokeniztion issues, so this way you are sure the file is really untokenized.\n!python3 MT-Preparation/subwording/3-desubword.py /kaggle/input/nmt-data-subword/target.model /kaggle/input/nmt-data-subword/split_val2023_10kto100k.lo.subword","metadata":{"id":"kOUWB4r3OFOV","outputId":"c16d31db-0643-4e05-c72a-f3982b4c6ebb","execution":{"iopub.status.busy":"2023-11-08T06:53:32.204167Z","iopub.execute_input":"2023-11-08T06:53:32.205025Z","iopub.status.idle":"2023-11-08T06:53:33.384788Z","shell.execute_reply.started":"2023-11-08T06:53:32.204994Z","shell.execute_reply":"2023-11-08T06:53:33.383837Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/kaggle/working/MT-Preparation/subwording/3-desubword.py\", line 21, in <module>\n    with open(target_pred) as pred, open(target_decodeded, \"w+\") as pred_decoded:\nOSError: [Errno 30] Read-only file system: '/kaggle/input/nmt-data-subword/split_val2023_10kto100k.lo.subword.desubword'\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the first 5 lines of the desubworded reference\n!head -n 5 split_val2023_10kto100k.lo.subword.desubword","metadata":{"id":"0jULN0MwOFeH","outputId":"c240f715-d7f8-451f-9bf6-1cf344af6810","execution":{"iopub.status.busy":"2023-11-08T06:40:39.063537Z","iopub.execute_input":"2023-11-08T06:40:39.063871Z","iopub.status.idle":"2023-11-08T06:40:40.019639Z","shell.execute_reply.started":"2023-11-08T06:40:39.063840Z","shell.execute_reply":"2023-11-08T06:40:40.018444Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"head: cannot open 'split_val2023_10kto100k.lo.subword.desubword' for reading: No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# MT Evaluation\n\nThere are several MT Evaluation metrics such as BLEU, TER, METEOR, COMET, BERTScore, among others.\n\nHere we are using BLEU. Files must be detokenized/desubworded beforehand.","metadata":{"id":"bHMumxqvLDDc"}},{"cell_type":"code","source":"# Download the BLEU script\n!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py","metadata":{"id":"w-9XGYnaJ-Nj","execution":{"iopub.status.busy":"2023-11-08T06:52:25.541086Z","iopub.execute_input":"2023-11-08T06:52:25.541518Z","iopub.status.idle":"2023-11-08T06:52:26.700711Z","shell.execute_reply.started":"2023-11-08T06:52:25.541484Z","shell.execute_reply":"2023-11-08T06:52:26.699577Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"--2023-11-08 06:52:26--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 957 [text/plain]\nSaving to: ‘compute-bleu.py.1’\n\ncompute-bleu.py.1   100%[===================>]     957  --.-KB/s    in 0s      \n\n2023-11-08 06:52:26 (67.7 MB/s) - ‘compute-bleu.py.1’ saved [957/957]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install sacrebleu\n!pip3 install sacrebleu","metadata":{"id":"rYDG0x0KLk_O","execution":{"iopub.status.busy":"2023-11-08T06:52:46.992402Z","iopub.execute_input":"2023-11-08T06:52:46.992687Z","iopub.status.idle":"2023-11-08T06:52:58.457588Z","shell.execute_reply.started":"2023-11-08T06:52:46.992661Z","shell.execute_reply":"2023-11-08T06:52:58.456481Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.3.2)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.8.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.6.3)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the translation (without subwording)\n!python3 compute-bleu.py /kaggle/input/valtest-nmt/split_val2023_10kto100k.lo UN.lo.translated.desubword","metadata":{"id":"W3V3tZphTzK9","outputId":"c2a85bb9-9a25-420e-98fd-800a554aae79","execution":{"iopub.status.busy":"2023-11-08T07:01:41.341587Z","iopub.execute_input":"2023-11-08T07:01:41.342307Z","iopub.status.idle":"2023-11-08T07:01:45.081322Z","shell.execute_reply.started":"2023-11-08T07:01:41.342270Z","shell.execute_reply":"2023-11-08T07:01:45.080112Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Reference 1st sentence: ພ້ອມກັນນັ້ນ, ຍູ້ແຮງການຮ່ວມມືຕໍ່ສູ້ປ້ອງກັນ, ຕ້ານອາດຊະຍາກໍາ; ເພີ່ມທະວີການຮ່ວມມືບໍາລຸງສ້າງ ລະຫວ່າງ 2 ຝ່າຍ ເພື່ອແນໃສ່ຍົກສູງກໍາລັງຄວາມສາມາດໃຫ້ພະນັກງານນັກຮົບໃນກະຊວງຕໍາຫຼວດ ຫວຽດນາມ, ໂດຍສະເພາະແມ່ນໃນຂົງເຂດປ້ອງກັນ, ຕ້ານການອາດຊະຍາກໍາ, ເຕັກນິກທາງອາຍາ, ຄວາມປອດໄພໃນການຈະລາຈອນ.\nMTed 1st sentence: ພ້ອມກັນນັ້ນ, ກະຊວງກະສິກໍາ ແລະ ພັດທະນາຊົນນະບົດ ຫວຽດນາມ ກໍ່ເພີ່ມທະວີວຽກງານຈັດຕັ້ງການຮັບທາບ, ພິເສດແມ່ນ ພາຍ 2 ປະເທດ, ພິເສດແມ່ນພະຍຸ, ປ້ອງກັນ, ຕ້ານບັນດາຂໍ້ກໍານົດ, ຕ້ານເຊື້ອພະຍາດຕິດຕໍ່, ພິເສດແມ່ນວິສາຫະກິດ...\nBLEU:  15.905601286465224\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# More Features and Directions to Explore\n\nExperiment with the following ideas:\n* Icrease `train_steps` and see to what extent new checkpoints provide better translation, in terms of both BLEU and your human evaluation.\n\n* Check other MT Evaluation mentrics other than BLEU such as [TER](https://github.com/mjpost/sacrebleu#ter), [WER](https://blog.machinetranslation.io/compute-wer-score/), [METEOR](https://blog.machinetranslation.io/compute-bleu-score/#meteor), [COMET](https://github.com/Unbabel/COMET), and [BERTScore](https://github.com/Tiiiger/bert_score). What are the conceptual differences between them? Is there special cases for using a specific metric?\n\n* Continue training from the last model checkpoint using the `-train_from` option, only if the training stopped and you want to continue it. In this case, `train_steps` in the config file should be larger than the steps of the last checkpoint you train from.\n```\n!onmt_train -config config.yaml -train_from models/model.fren_step_3000.pt\n```\n\n* **Ensemble Decoding:** During translation, instead of adding one model/checkpoint to the `-model` argument, add multiple checkpoints. For example, try the two last checkpoints. Does it improve quality of translation? Does it affect translation seepd?\n\n* **Averaging Models:** Try to average multiple models into one model using the [average_models.py](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/bin/average_models.py) script, and see how this affects translation quality.\n```\npython3 average_models.py -models model_step_xxx.pt model_step_yyy.pt -output model_avg.pt\n```\n* **Release the model:** Try this command and see how it reduce the model size.\n```\nonmt_release_model --model \"model.pt\" --output \"model_released.pt\n```\n* **Use CTranslate2:** For efficient translation, consider using [CTranslate2](https://github.com/OpenNMT/CTranslate2), a fast inference engine. Check out an [example](https://gist.github.com/ymoslem/60e1d1dc44fe006f67e130b6ad703c4b).\n\n* **Work on low-resource languages:** Find out more details about [how to train NMT models for low-resource languages](https://blog.machinetranslation.io/low-resource-nmt/).\n\n* **Train a multilingual model:** Find out helpful notes about [training multilingual models](https://blog.machinetranslation.io/multilingual-nmt).\n\n* **Publish a demo:** Show off your work through a [simple demo with CTranslate2 and Streamlit](https://blog.machinetranslation.io/nmt-web-interface/).\n","metadata":{"id":"IBi1PhRv4bX9"}}]}