{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/ymoslem/OpenNMT-Tutorial/blob/main/2-NMT-Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"# Install OpenNMT-py 3.x\n!pip3 install OpenNMT-py==3.4","metadata":{"id":"vSUyCs23M_H2","execution":{"iopub.status.busy":"2023-11-08T06:39:13.615568Z","iopub.execute_input":"2023-11-08T06:39:13.615857Z","iopub.status.idle":"2023-11-08T06:39:30.410405Z","shell.execute_reply.started":"2023-11-08T06:39:13.615831Z","shell.execute_reply":"2023-11-08T06:39:30.409455Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting OpenNMT-py==3.4\n  Downloading OpenNMT_py-3.4-py3-none-any.whl (250 kB)\n\u001b[2K     \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m250.5/250.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch<2.1,>=1.13 in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (2.0.0)\nCollecting configargparse (from OpenNMT-py==3.4)\n  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\nCollecting ctranslate2<4,>=3.2 (from OpenNMT-py==3.4)\n  Downloading ctranslate2-3.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n\u001b[2K     \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (2.12.3)\nRequirement already satisfied: flask in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (2.3.3)\nCollecting waitress (from OpenNMT-py==3.4)\n  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py==3.4)\n  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n\u001b[2K     \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (6.0)\nCollecting sacrebleu (from OpenNMT-py==3.4)\n  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n\u001b[2K     \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: rapidfuzz in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (3.2.0)\nCollecting pyahocorasick (from OpenNMT-py==3.4)\n  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K     \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py==3.4)\n  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n\u001b[2K     \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (from OpenNMT-py==3.4) (3.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ctranslate2<4,>=3.2->OpenNMT-py==3.4) (1.23.5)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (68.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (2.3.7)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.3->OpenNMT-py==3.4) (0.40.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.13->OpenNMT-py==3.4) (3.1.2)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext-wheel->OpenNMT-py==3.4) (2.11.1)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->OpenNMT-py==3.4) (2.1.2)\nRequirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from flask->OpenNMT-py==3.4) (8.1.7)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask->OpenNMT-py==3.4) (1.6.2)\nCollecting portalocker (from sacrebleu->OpenNMT-py==3.4)\n  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu->OpenNMT-py==3.4) (2023.6.3)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->OpenNMT-py==3.4) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->OpenNMT-py==3.4) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->OpenNMT-py==3.4) (4.9.3)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (0.10.1)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (4.66.1)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (1.10.9)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy->OpenNMT-py==3.4) (3.3.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (4.9)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py==3.4) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<2.1,>=1.13->OpenNMT-py==3.4) (2.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy->OpenNMT-py==3.4) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py==3.4) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py==3.4) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py==3.4) (2023.7.22)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py==3.4) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py==3.4) (0.1.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<2.1,>=1.13->OpenNMT-py==3.4) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py==3.4) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py==3.4) (3.2.2)\nInstalling collected packages: waitress, pyonmttok, pyahocorasick, portalocker, fasttext-wheel, ctranslate2, configargparse, sacrebleu, OpenNMT-py\nSuccessfully installed OpenNMT-py-3.4 configargparse-1.7 ctranslate2-3.20.0 fasttext-wheel-0.9.2 portalocker-2.8.2 pyahocorasick-2.0.0 pyonmttok-1.37.1 sacrebleu-2.3.2 waitress-2.1.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare Your Datasets\nPlease make sure you have completed the [first exercise](https://colab.research.google.com/drive/1rsFPnAQu9-_A6e2Aw9JYK3C8mXx9djsF?usp=sharing).","metadata":{"id":"vhgIdJn-cLqu"}},{"cell_type":"code","source":"# Open the folder where you saved your prepapred datasets from the first exercise\n# You might need to mount your Google Drive first\n# %cd /content/drive/MyDrive/nmt/\n# !ls","metadata":{"id":"dWVOWYedzZ_G","execution":{"iopub.status.busy":"2023-11-08T06:39:30.412266Z","iopub.execute_input":"2023-11-08T06:39:30.412556Z","iopub.status.idle":"2023-11-08T06:39:30.416766Z","shell.execute_reply.started":"2023-11-08T06:39:30.412527Z","shell.execute_reply":"2023-11-08T06:39:30.415916Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Create the Training Configuration File\n\nThe following config file matches most of the recommended values for the Transformer model [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). As the current dataset is small, we reduced the following values: \n* `train_steps` - for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more! Enabling the option `early_stopping` can help stop the training when there is no considerable improvement.\n* `valid_steps` - 10000 can be good if the value `train_steps` is big enough. \n* `warmup_steps` - obviously, its value must be less than `train_steps`. Try 4000 and 8000 values.\n\nRefer to [OpenNMT-py training parameters](https://opennmt.net/OpenNMT-py/options/train.html) for more details. If you are interested in further explanation of the Transformer model, you can check this article, [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).","metadata":{"id":"MPlmhd426B7l"}},{"cell_type":"code","source":"# Create the YAML configuration file\n# On a regular machine, you can create it manually or with nano\n# Note here we are using some smaller values because the dataset is small\n# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n\nconfig = '''# config.yaml\n\n\n## Where the samples will be written\nsave_data: run\n\n# Training files\ndata:\n    corpus_1:\n        path_src: /kaggle/input/nmt-data-subword/split_train2023_10kto100k.vi.subword\n        path_tgt: /kaggle/input/nmt-data-subword/split_train2023_10kto100k.lo.subword\n        transforms: [filtertoolong]\n    valid:\n        path_src: /kaggle/input/nmt-data-subword/split_val2023_10kto100k.vi.subword\n        path_tgt: /kaggle/input/nmt-data-subword/split_val2023_10kto100k.lo.subword\n        transforms: [filtertoolong]\n\n# Vocabulary files, generated by onmt_build_vocab\nsrc_vocab: run/source.vocab\ntgt_vocab: run/target.vocab\n\n# Vocabulary size - should be the same as in sentence piece\nsrc_vocab_size: 50000\ntgt_vocab_size: 50000\n\n# Filter out source/target longer than n if [filtertoolong] enabled\nsrc_seq_length: 150\nsrc_seq_length: 150\n\n# Tokenization options\nsrc_subword_model: /kaggle/input/nmt-data-subword/source.model\ntgt_subword_model: /kaggle/input/nmt-data-subword/target.model\n\n# Where to save the log file and the output models/checkpoints\nlog_file: train.log\nsave_model: models/model.vilo\n\n# Stop training if it does not imporve after n validations\nearly_stopping: 4\n\n# Default: 5000 - Save a model checkpoint for each n\nsave_checkpoint_steps: 5000\n\n# To save space, limit checkpoints to last n\n# keep_checkpoint: 3\n\nseed: 3435\n\n# Default: 100000 - Train the model to max n steps \n# Increase to 200000 or more for large datasets\n# For fine-tuning, add up the required steps to the original steps\ntrain_steps: 100000\n\n# Default: 10000 - Run validation after n steps\nvalid_steps: 1000\n\n# Default: 4000 - for large datasets, try up to 8000\nwarmup_steps: 1000\nreport_every: 100\n\n# Number of GPUs, and IDs of GPUs\nworld_size: 1\ngpu_ranks: [0]\n\n# Batching\nbucket_size: 262144\nnum_workers: 0  # Default: 2, set to 0 when RAM out of memory\nbatch_type: \"tokens\"\nbatch_size: 4096   # Tokens per batch, change when CUDA out of memory\nvalid_batch_size: 2048\nmax_generator_batches: 2\naccum_count: [4]\naccum_steps: [0]\n\n# Optimization\nmodel_dtype: \"fp16\"\noptim: \"adam\"\nlearning_rate: 2\n# warmup_steps: 8000\ndecay_method: \"noam\"\nadam_beta2: 0.998\nmax_grad_norm: 0\nlabel_smoothing: 0.1\nparam_init: 0\nparam_init_glorot: true\nnormalization: \"tokens\"\n\n# Model\nencoder_type: transformer\ndecoder_type: transformer\nposition_encoding: true\nenc_layers: 6\ndec_layers: 6\nheads: 8\nhidden_size: 512\nword_vec_size: 512\ntransformer_ff: 2048\ndropout_steps: [0]\ndropout: [0.1]\nattention_dropout: [0.1]\n'''\n\nwith open(\"config.yaml\", \"w+\") as config_yaml:\n  config_yaml.write(config)","metadata":{"id":"qbW7Xek6UDlY","execution":{"iopub.status.busy":"2023-11-08T06:39:30.418047Z","iopub.execute_input":"2023-11-08T06:39:30.418322Z","iopub.status.idle":"2023-11-08T06:39:30.429404Z","shell.execute_reply.started":"2023-11-08T06:39:30.418300Z","shell.execute_reply":"2023-11-08T06:39:30.428712Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# [Optional] Check the content of the configuration file\n!cat config.yaml","metadata":{"id":"vsL4zycvLMUx","execution":{"iopub.status.busy":"2023-11-08T06:39:30.431714Z","iopub.execute_input":"2023-11-08T06:39:30.431984Z","iopub.status.idle":"2023-11-08T06:39:31.407619Z","shell.execute_reply.started":"2023-11-08T06:39:30.431962Z","shell.execute_reply":"2023-11-08T06:39:31.406493Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"# config.yaml\n\n\n## Where the samples will be written\nsave_data: run\n\n# Training files\ndata:\n    corpus_1:\n        path_src: /kaggle/input/nmt-data-subword/split_train2023_10kto100k.vi.subword\n        path_tgt: /kaggle/input/nmt-data-subword/split_train2023_10kto100k.lo.subword\n        transforms: [filtertoolong]\n    valid:\n        path_src: /kaggle/input/nmt-data-subword/split_val2023_10kto100k.vi.subword\n        path_tgt: /kaggle/input/nmt-data-subword/split_val2023_10kto100k.lo.subword\n        transforms: [filtertoolong]\n\n# Vocabulary files, generated by onmt_build_vocab\nsrc_vocab: run/source.vocab\ntgt_vocab: run/target.vocab\n\n# Vocabulary size - should be the same as in sentence piece\nsrc_vocab_size: 50000\ntgt_vocab_size: 50000\n\n# Filter out source/target longer than n if [filtertoolong] enabled\nsrc_seq_length: 150\nsrc_seq_length: 150\n\n# Tokenization options\nsrc_subword_model: /kaggle/input/nmt-data-subword/source.model\ntgt_subword_model: /kaggle/input/nmt-data-subword/target.model\n\n# Where to save the log file and the output models/checkpoints\nlog_file: train.log\nsave_model: models/model.fren\n\n# Stop training if it does not imporve after n validations\nearly_stopping: 4\n\n# Default: 5000 - Save a model checkpoint for each n\nsave_checkpoint_steps: 1000\n\n# To save space, limit checkpoints to last n\n# keep_checkpoint: 3\n\nseed: 3435\n\n# Default: 100000 - Train the model to max n steps \n# Increase to 200000 or more for large datasets\n# For fine-tuning, add up the required steps to the original steps\ntrain_steps: 3000\n\n# Default: 10000 - Run validation after n steps\nvalid_steps: 1000\n\n# Default: 4000 - for large datasets, try up to 8000\nwarmup_steps: 1000\nreport_every: 100\n\n# Number of GPUs, and IDs of GPUs\nworld_size: 1\ngpu_ranks: [0]\n\n# Batching\nbucket_size: 262144\nnum_workers: 0  # Default: 2, set to 0 when RAM out of memory\nbatch_type: \"tokens\"\nbatch_size: 4096   # Tokens per batch, change when CUDA out of memory\nvalid_batch_size: 2048\nmax_generator_batches: 2\naccum_count: [4]\naccum_steps: [0]\n\n# Optimization\nmodel_dtype: \"fp16\"\noptim: \"adam\"\nlearning_rate: 2\n# warmup_steps: 8000\ndecay_method: \"noam\"\nadam_beta2: 0.998\nmax_grad_norm: 0\nlabel_smoothing: 0.1\nparam_init: 0\nparam_init_glorot: true\nnormalization: \"tokens\"\n\n# Model\nencoder_type: transformer\ndecoder_type: transformer\nposition_encoding: true\nenc_layers: 6\ndec_layers: 6\nheads: 8\nhidden_size: 512\nword_vec_size: 512\ntransformer_ff: 2048\ndropout_steps: [0]\ndropout: [0.1]\nattention_dropout: [0.1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Build Vocabulary\n\nFor large datasets, it is not feasable to use all words/tokens found in the corpus. Instead, a specific set of vocabulary is extracted from the training dataset, usually betweeen 32k and 100k words. This is the main purpose of the vocabulary building step.","metadata":{"id":"F0bcqYkEXhRY"}},{"cell_type":"code","source":"# Find the number of CPUs/cores on the machine\n!nproc --all","metadata":{"id":"AuwltKp_VhnQ","outputId":"4d9d5e5e-df7b-474b-b281-369424c47603","execution":{"iopub.status.busy":"2023-11-08T06:39:31.409067Z","iopub.execute_input":"2023-11-08T06:39:31.409390Z","iopub.status.idle":"2023-11-08T06:39:32.372769Z","shell.execute_reply.started":"2023-11-08T06:39:31.409359Z","shell.execute_reply":"2023-11-08T06:39:32.371669Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}]},{"cell_type":"code","source":"# Build Vocabulary\n\n# -config: path to your config.yaml file\n# -n_sample: use -1 to build vocabulary on all the segment in the training dataset\n# -num_threads: change it to match the number of CPUs to run it faster\n\n!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 4","metadata":{"id":"P2GV1PgyUsJr","outputId":"e1749505-d1f6-41cd-9420-1876db27e405","execution":{"iopub.status.busy":"2023-11-08T06:39:32.374310Z","iopub.execute_input":"2023-11-08T06:39:32.374606Z","iopub.status.idle":"2023-11-08T06:39:53.617872Z","shell.execute_reply.started":"2023-11-08T06:39:32.374577Z","shell.execute_reply":"2023-11-08T06:39:53.616757Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-11-08 06:39:48,531 INFO] Counter vocab from -1 samples.\n[2023-11-08 06:39:48,531 INFO] n_sample=-1: Build vocab on full datasets.\n[2023-11-08 06:39:50,840 INFO] * Transform statistics for corpus_1(25.00%):\n\t\t\t* FilterTooLongStats(filtered=409)\n\n[2023-11-08 06:39:50,844 INFO] * Transform statistics for corpus_1(25.00%):\n\t\t\t* FilterTooLongStats(filtered=440)\n\n[2023-11-08 06:39:50,846 INFO] * Transform statistics for corpus_1(25.00%):\n\t\t\t* FilterTooLongStats(filtered=461)\n\n[2023-11-08 06:39:50,848 INFO] * Transform statistics for corpus_1(25.00%):\n\t\t\t* FilterTooLongStats(filtered=384)\n\n[2023-11-08 06:39:50,989 INFO] Counters src: 12288\n[2023-11-08 06:39:50,990 INFO] Counters tgt: 27900\n","output_type":"stream"}]},{"cell_type":"markdown","source":"From the **Runtime menu** > **Change runtime type**, make sure that the \"**Hardware accelerator**\" is \"**GPU**\".\n","metadata":{"id":"ncWyNtxiO_Ov"}},{"cell_type":"code","source":"# Check if the GPU is active\n!nvidia-smi -L","metadata":{"id":"TMMPeS-pSV8I","outputId":"ea51133a-beaa-4642-e8ba-7bd9159ada68","execution":{"iopub.status.busy":"2023-11-08T06:39:53.619380Z","iopub.execute_input":"2023-11-08T06:39:53.619714Z","iopub.status.idle":"2023-11-08T06:39:54.590174Z","shell.execute_reply.started":"2023-11-08T06:39:53.619683Z","shell.execute_reply":"2023-11-08T06:39:54.588937Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-ce52e806-e7e0-68d5-bbb2-24ed59359682)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check if the GPU is visable to PyTorch\n\nimport torch\n\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0))\n\ngpu_memory = torch.cuda.mem_get_info(0)\nprint(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)","metadata":{"id":"_3rVQhd4NXNG","outputId":"181eb6a3-cc09-45b6-de4e-b1e88e45f97b","execution":{"iopub.status.busy":"2023-11-08T06:39:54.591966Z","iopub.execute_input":"2023-11-08T06:39:54.592867Z","iopub.status.idle":"2023-11-08T06:39:58.999352Z","shell.execute_reply.started":"2023-11-08T06:39:54.592824Z","shell.execute_reply":"2023-11-08T06:39:58.998207Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"True\nTesla P100-PCIE-16GB\nFree GPU memory: 15665.75 out of: 16280.875\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training\n\nNow, start training your NMT model! ๐ ๐ ๐","metadata":{"id":"8aCxETSnXcL-"}},{"cell_type":"code","source":"!rm -rf /kaggle/working/nmt/models/","metadata":{"id":"HZd1o1kIb6Nv","execution":{"iopub.status.busy":"2023-11-08T06:39:59.000793Z","iopub.execute_input":"2023-11-08T06:39:59.001300Z","iopub.status.idle":"2023-11-08T06:40:00.009639Z","shell.execute_reply.started":"2023-11-08T06:39:59.001272Z","shell.execute_reply":"2023-11-08T06:40:00.008358Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Train the NMT model\n!onmt_train -config config.yaml","metadata":{"id":"prJCKA2CP-dl","execution":{"iopub.status.busy":"2023-11-08T06:40:00.013228Z","iopub.execute_input":"2023-11-08T06:40:00.013540Z","iopub.status.idle":"2023-11-08T06:40:12.257841Z","shell.execute_reply.started":"2023-11-08T06:40:00.013511Z","shell.execute_reply":"2023-11-08T06:40:12.256635Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-11-08 06:40:08,149 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n[2023-11-08 06:40:08,150 INFO] Parsed 2 corpora from -data.\n[2023-11-08 06:40:08,150 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n[2023-11-08 06:40:08,345 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'โ', ',', '0', '.', '1', '2']\n[2023-11-08 06:40:08,345 INFO] The decoder start token is: <s>\n[2023-11-08 06:40:08,345 INFO] Building model...\n[2023-11-08 06:40:09,407 INFO] Switching model to float32 for amp/apex_amp\n[2023-11-08 06:40:09,407 INFO] Non quantized layer compute is fp16\n^C\n","output_type":"stream"}]},{"cell_type":"code","source":"# For error debugging try:\n# !dmesg -T","metadata":{"id":"XUYAvE8ffK2k","execution":{"iopub.status.busy":"2023-11-08T06:40:12.259232Z","iopub.execute_input":"2023-11-08T06:40:12.259561Z","iopub.status.idle":"2023-11-08T06:40:12.264486Z","shell.execute_reply.started":"2023-11-08T06:40:12.259532Z","shell.execute_reply":"2023-11-08T06:40:12.263263Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Translation\n\nTranslation Options:\n* `-model` - specify the last model checkpoint name; try testing the quality of multiple checkpoints\n* `-src` - the subworded test dataset, source file\n* `-output` - give any file name to the new translation output file\n* `-gpu` - GPU ID, usually 0 if you have one GPU. Otherwise, it will translate on CPU, which would be slower.\n* `-min_length` - [optional] to avoid empty translations\n* `-verbose` - [optional] if you want to print translations\n\nRefer to [OpenNMT-py translation options](https://opennmt.net/OpenNMT-py/options/translate.html) for more details.","metadata":{"id":"eShpS01j-Jcp"}},{"cell_type":"code","source":"!git clone https://github.com/ymoslem/MT-Preparation.git","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:40:54.928415Z","iopub.execute_input":"2023-11-08T06:40:54.928732Z","iopub.status.idle":"2023-11-08T06:40:55.898019Z","shell.execute_reply.started":"2023-11-08T06:40:54.928694Z","shell.execute_reply":"2023-11-08T06:40:55.896723Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"fatal: destination path 'MT-Preparation' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Translate the \"subworded\" source file of the test dataset\n# Change the model name, if needed.\n!onmt_translate -model models/model.vilo_step_100000.pt -src /kaggle/input/nmt-data-subword/split_val2023_10kto100k.vi.subword -output UN.lo.translated -gpu 0 -min_length 1","metadata":{"id":"MbQEGTj4TybH","outputId":"b2181f89-47a1-46d3-888c-9facf296bf61","execution":{"iopub.status.busy":"2023-11-08T06:41:08.285316Z","iopub.execute_input":"2023-11-08T06:41:08.286510Z","iopub.status.idle":"2023-11-08T06:49:23.758537Z","shell.execute_reply.started":"2023-11-08T06:41:08.286463Z","shell.execute_reply":"2023-11-08T06:49:23.757462Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-11-08 06:41:15,967 INFO] Loading checkpoint from /kaggle/input/result-model-nmt/model.fren_step_3000.pt\n[2023-11-08 06:41:23,487 INFO] Loading data into the model\n[2023-11-08 06:49:22,274 INFO] PRED SCORE: -0.4574, PRED PPL: 1.58 NB SENTENCES: 12000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the first 5 lines of the translation file\n!head -n 5 UN.lo.translated","metadata":{"id":"XHYihrgfIrIO","outputId":"980d6dad-3467-4157-a398-8fe05509cb54","execution":{"iopub.status.busy":"2023-11-08T06:50:04.095992Z","iopub.execute_input":"2023-11-08T06:50:04.096411Z","iopub.status.idle":"2023-11-08T06:50:05.053065Z","shell.execute_reply.started":"2023-11-08T06:50:04.096373Z","shell.execute_reply":"2023-11-08T06:50:05.051951Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"โเบเปเบญเบกเบเบฑเบ เบเบฑเปเบ , โ เบเบฐเบเบงเบเบเบฐเบชเบดเบเปเบฒ โเปเบฅเบฐ โเบเบฑเบเบเบฐเบเบฒเบเบปเบเบเบฐเบเบปเบ โเบซเบงเบฝเบเบเบฒเบก โเบเปเป เปเบเบตเปเบกเบเบฐเบงเบต เบงเบฝเบเบเบฒเบ เบเบฑเบเบเบฑเปเบ เบเบฒเบเบฎเบฑเบ เบเบฒ เบ , โเบเบดเปเบชเบเปเบกเปเบ โเบเบฒเบ โ 2 โเบเบฐเปเบเบ , โเบเบดเปเบชเบเปเบกเปเบ เบเบฐเบเบธ , โ เบเปเบญเบเบเบฑเบ , โเบเปเบฒเบ เบเบฑเบเบเบฒ เบเปเปเบเปเบฒเบเบปเบ , โเบเปเบฒเบ เปเบเบทเปเบญเบเบฐเบเบฒเบ เบเบดเบเบเปเป , โเบเบดเปเบชเบเปเบกเปเบ เบงเบดเบชเบฒเบซเบฐเบเบดเบ ...\nโเบเบฒเบ เบเปเบฅเบดเบชเบฑเบ เปเบเบฑเบ : โ+ โ 8 6 - 5 2 3 .\nโเบเปเบฒเบเบฐ โเปเบเบปเปเบฒ เบญเบฐเบเบดเบเบฒเบ โเบงเปเบฒ โเปเบ โเปเบฅเบ โเบเบตเป โเบเบฐ โเบเบฑเบเบฅเบธ โเบเบงเบฒเบก โเบชเปเบฒ โเปเบฅเบฑเบ , โเปเบฎเบปเบฒ โเบเบฐ โเปเบเป โเบฎเบฑเบ โเบเบงเบฒเบก โเบเปเบญเบก โเบเบปเบ โเปเบเบทเปเบญ โเบเบฐ โเปเบเป โเบฎเบฑเบ .\nโเปเบเบเบงเบฒเบกเปเบเบฑเบเบเบดเบ , โเบชเบฑเบเบเบปเบก เปเบเป เบเปเบฒเบเบปเบง เบเบฒเบ .\nโ 1 2 โเบเบฑเบ โเบเบตเป โเบซเบฅเบฒเบ โเบเบปเบ โเบเปเป โเปเบเป โเบเบฒเบ โเปเบ , โเปเบเป โเบฅเบนเบ โเบเบฑเบ โเปเบกเบ โเบเบญเบ โเบเบปเบ โเบญเบตเบ , โเปเบเป โเปเบเบปเบฒ โเปเบเป โเบเบฒเบ โเปเบ โเบเปเบงเบ โเบเบงเบฒเบก โเบเบฒเบ .\n","output_type":"stream"}]},{"cell_type":"code","source":"# If needed install/update sentencepiece\n!pip3 install --upgrade -q sentencepiece\n\n# Desubword the translation file\n!python3 MT-Preparation/subwording/3-desubword.py /kaggle/input/nmt-data-subword/target.model UN.lo.translated","metadata":{"id":"zRsJm6UET2C_","outputId":"f9a410d7-e753-4c43-e5cb-82c62ed00a53","execution":{"iopub.status.busy":"2023-11-08T06:51:30.703811Z","iopub.execute_input":"2023-11-08T06:51:30.704150Z","iopub.status.idle":"2023-11-08T06:51:43.828721Z","shell.execute_reply.started":"2023-11-08T06:51:30.704104Z","shell.execute_reply":"2023-11-08T06:51:43.827628Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Done desubwording! Output: UN.lo.translated.desubword\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the first 5 lines of the desubworded translation file\n!head -n 5 UN.lo.translated.desubword","metadata":{"id":"ai4RhhGaKBp1","outputId":"a833acd0-99d0-48ed-ad63-dac8f91ff4da","execution":{"iopub.status.busy":"2023-11-08T06:51:49.444459Z","iopub.execute_input":"2023-11-08T06:51:49.445100Z","iopub.status.idle":"2023-11-08T06:51:50.403578Z","shell.execute_reply.started":"2023-11-08T06:51:49.445064Z","shell.execute_reply":"2023-11-08T06:51:50.402575Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"เบเปเบญเบกเบเบฑเบเบเบฑเปเบ, เบเบฐเบเบงเบเบเบฐเบชเบดเบเปเบฒ เปเบฅเบฐ เบเบฑเบเบเบฐเบเบฒเบเบปเบเบเบฐเบเบปเบ เบซเบงเบฝเบเบเบฒเบก เบเปเปเปเบเบตเปเบกเบเบฐเบงเบตเบงเบฝเบเบเบฒเบเบเบฑเบเบเบฑเปเบเบเบฒเบเบฎเบฑเบเบเบฒเบ, เบเบดเปเบชเบเปเบกเปเบ เบเบฒเบ 2 เบเบฐเปเบเบ, เบเบดเปเบชเบเปเบกเปเบเบเบฐเบเบธ, เบเปเบญเบเบเบฑเบ, เบเปเบฒเบเบเบฑเบเบเบฒเบเปเปเบเปเบฒเบเบปเบ, เบเปเบฒเบเปเบเบทเปเบญเบเบฐเบเบฒเบเบเบดเบเบเปเป, เบเบดเปเบชเบเปเบกเปเบเบงเบดเบชเบฒเบซเบฐเบเบดเบ...\nเบเบฒเบเบเปเบฅเบดเบชเบฑเบเปเบเบฑเบ: + 86-523.\nเบเปเบฒเบเบฐ เปเบเบปเปเบฒเบญเบฐเบเบดเบเบฒเบ เบงเปเบฒ เปเบ เปเบฅเบ เบเบตเป เบเบฐ เบเบฑเบเบฅเบธ เบเบงเบฒเบก เบชเปเบฒ เปเบฅเบฑเบ, เปเบฎเบปเบฒ เบเบฐ เปเบเป เบฎเบฑเบ เบเบงเบฒเบก เบเปเบญเบก เบเบปเบ เปเบเบทเปเบญ เบเบฐ เปเบเป เบฎเบฑเบ.\nเปเบเบเบงเบฒเบกเปเบเบฑเบเบเบดเบ, เบชเบฑเบเบเบปเบกเปเบเปเบเปเบฒเบเบปเบงเบเบฒเบ.\n12 เบเบฑเบ เบเบตเป เบซเบฅเบฒเบ เบเบปเบ เบเปเป เปเบเป เบเบฒเบ เปเบ, เปเบเป เบฅเบนเบ เบเบฑเบ เปเบกเบ เบเบญเบ เบเบปเบ เบญเบตเบ, เปเบเป เปเบเบปเบฒ เปเบเป เบเบฒเบ เปเบ เบเปเบงเบ เบเบงเบฒเบก เบเบฒเบ.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Desubword the target file (reference) of the test dataset\n# Note: You might as well have split files *before* subwording during dataset preperation, \n# but sometimes datasets have tokeniztion issues, so this way you are sure the file is really untokenized.\n!python3 MT-Preparation/subwording/3-desubword.py /kaggle/input/nmt-data-subword/target.model /kaggle/input/nmt-data-subword/split_val2023_10kto100k.lo.subword","metadata":{"id":"kOUWB4r3OFOV","outputId":"c16d31db-0643-4e05-c72a-f3982b4c6ebb","execution":{"iopub.status.busy":"2023-11-08T06:53:32.204167Z","iopub.execute_input":"2023-11-08T06:53:32.205025Z","iopub.status.idle":"2023-11-08T06:53:33.384788Z","shell.execute_reply.started":"2023-11-08T06:53:32.204994Z","shell.execute_reply":"2023-11-08T06:53:33.383837Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/kaggle/working/MT-Preparation/subwording/3-desubword.py\", line 21, in <module>\n    with open(target_pred) as pred, open(target_decodeded, \"w+\") as pred_decoded:\nOSError: [Errno 30] Read-only file system: '/kaggle/input/nmt-data-subword/split_val2023_10kto100k.lo.subword.desubword'\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the first 5 lines of the desubworded reference\n!head -n 5 split_val2023_10kto100k.lo.subword.desubword","metadata":{"id":"0jULN0MwOFeH","outputId":"c240f715-d7f8-451f-9bf6-1cf344af6810","execution":{"iopub.status.busy":"2023-11-08T06:40:39.063537Z","iopub.execute_input":"2023-11-08T06:40:39.063871Z","iopub.status.idle":"2023-11-08T06:40:40.019639Z","shell.execute_reply.started":"2023-11-08T06:40:39.063840Z","shell.execute_reply":"2023-11-08T06:40:40.018444Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"head: cannot open 'split_val2023_10kto100k.lo.subword.desubword' for reading: No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# MT Evaluation\n\nThere are several MT Evaluation metrics such as BLEU, TER, METEOR, COMET, BERTScore, among others.\n\nHere we are using BLEU. Files must be detokenized/desubworded beforehand.","metadata":{"id":"bHMumxqvLDDc"}},{"cell_type":"code","source":"# Download the BLEU script\n!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py","metadata":{"id":"w-9XGYnaJ-Nj","execution":{"iopub.status.busy":"2023-11-08T06:52:25.541086Z","iopub.execute_input":"2023-11-08T06:52:25.541518Z","iopub.status.idle":"2023-11-08T06:52:26.700711Z","shell.execute_reply.started":"2023-11-08T06:52:25.541484Z","shell.execute_reply":"2023-11-08T06:52:26.699577Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"--2023-11-08 06:52:26--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 957 [text/plain]\nSaving to: โcompute-bleu.py.1โ\n\ncompute-bleu.py.1   100%[===================>]     957  --.-KB/s    in 0s      \n\n2023-11-08 06:52:26 (67.7 MB/s) - โcompute-bleu.py.1โ saved [957/957]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install sacrebleu\n!pip3 install sacrebleu","metadata":{"id":"rYDG0x0KLk_O","execution":{"iopub.status.busy":"2023-11-08T06:52:46.992402Z","iopub.execute_input":"2023-11-08T06:52:46.992687Z","iopub.status.idle":"2023-11-08T06:52:58.457588Z","shell.execute_reply.started":"2023-11-08T06:52:46.992661Z","shell.execute_reply":"2023-11-08T06:52:58.456481Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.3.2)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.8.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.6.3)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the translation (without subwording)\n!python3 compute-bleu.py /kaggle/input/valtest-nmt/split_val2023_10kto100k.lo UN.lo.translated.desubword","metadata":{"id":"W3V3tZphTzK9","outputId":"c2a85bb9-9a25-420e-98fd-800a554aae79","execution":{"iopub.status.busy":"2023-11-08T07:01:41.341587Z","iopub.execute_input":"2023-11-08T07:01:41.342307Z","iopub.status.idle":"2023-11-08T07:01:45.081322Z","shell.execute_reply.started":"2023-11-08T07:01:41.342270Z","shell.execute_reply":"2023-11-08T07:01:45.080112Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Reference 1st sentence: เบเปเบญเบกเบเบฑเบเบเบฑเปเบ, เบเบนเปเปเบฎเบเบเบฒเบเบฎเปเบงเบกเบกเบทเบเปเปเบชเบนเปเบเปเบญเบเบเบฑเบ, เบเปเบฒเบเบญเบฒเบเบเบฐเบเบฒเบเปเบฒ; เปเบเบตเปเบกเบเบฐเบงเบตเบเบฒเบเบฎเปเบงเบกเบกเบทเบเปเบฒเบฅเบธเบเบชเปเบฒเบ เบฅเบฐเบซเบงเปเบฒเบ 2 เบเปเบฒเบ เปเบเบทเปเบญเปเบเปเบชเปเบเบปเบเบชเบนเบเบเปเบฒเบฅเบฑเบเบเบงเบฒเบกเบชเบฒเบกเบฒเบเปเบซเปเบเบฐเบเบฑเบเบเบฒเบเบเบฑเบเบฎเบปเบเปเบเบเบฐเบเบงเบเบเปเบฒเบซเบผเบงเบ เบซเบงเบฝเบเบเบฒเบก, เปเบเบเบชเบฐเปเบเบฒเบฐเปเบกเปเบเปเบเบเบปเบเปเบเบเบเปเบญเบเบเบฑเบ, เบเปเบฒเบเบเบฒเบเบญเบฒเบเบเบฐเบเบฒเบเปเบฒ, เปเบเบฑเบเบเบดเบเบเบฒเบเบญเบฒเบเบฒ, เบเบงเบฒเบกเบเบญเบเปเบเปเบเบเบฒเบเบเบฐเบฅเบฒเบเบญเบ.\nMTed 1st sentence: เบเปเบญเบกเบเบฑเบเบเบฑเปเบ, เบเบฐเบเบงเบเบเบฐเบชเบดเบเปเบฒ เปเบฅเบฐ เบเบฑเบเบเบฐเบเบฒเบเบปเบเบเบฐเบเบปเบ เบซเบงเบฝเบเบเบฒเบก เบเปเปเปเบเบตเปเบกเบเบฐเบงเบตเบงเบฝเบเบเบฒเบเบเบฑเบเบเบฑเปเบเบเบฒเบเบฎเบฑเบเบเบฒเบ, เบเบดเปเบชเบเปเบกเปเบ เบเบฒเบ 2 เบเบฐเปเบเบ, เบเบดเปเบชเบเปเบกเปเบเบเบฐเบเบธ, เบเปเบญเบเบเบฑเบ, เบเปเบฒเบเบเบฑเบเบเบฒเบเปเปเบเปเบฒเบเบปเบ, เบเปเบฒเบเปเบเบทเปเบญเบเบฐเบเบฒเบเบเบดเบเบเปเป, เบเบดเปเบชเบเปเบกเปเบเบงเบดเบชเบฒเบซเบฐเบเบดเบ...\nBLEU:  15.905601286465224\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# More Features and Directions to Explore\n\nExperiment with the following ideas:\n* Icrease `train_steps` and see to what extent new checkpoints provide better translation, in terms of both BLEU and your human evaluation.\n\n* Check other MT Evaluation mentrics other than BLEU such as [TER](https://github.com/mjpost/sacrebleu#ter), [WER](https://blog.machinetranslation.io/compute-wer-score/), [METEOR](https://blog.machinetranslation.io/compute-bleu-score/#meteor), [COMET](https://github.com/Unbabel/COMET), and [BERTScore](https://github.com/Tiiiger/bert_score). What are the conceptual differences between them? Is there special cases for using a specific metric?\n\n* Continue training from the last model checkpoint using the `-train_from` option, only if the training stopped and you want to continue it. In this case, `train_steps` in the config file should be larger than the steps of the last checkpoint you train from.\n```\n!onmt_train -config config.yaml -train_from models/model.fren_step_3000.pt\n```\n\n* **Ensemble Decoding:** During translation, instead of adding one model/checkpoint to the `-model` argument, add multiple checkpoints. For example, try the two last checkpoints. Does it improve quality of translation? Does it affect translation seepd?\n\n* **Averaging Models:** Try to average multiple models into one model using the [average_models.py](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/bin/average_models.py) script, and see how this affects translation quality.\n```\npython3 average_models.py -models model_step_xxx.pt model_step_yyy.pt -output model_avg.pt\n```\n* **Release the model:** Try this command and see how it reduce the model size.\n```\nonmt_release_model --model \"model.pt\" --output \"model_released.pt\n```\n* **Use CTranslate2:** For efficient translation, consider using [CTranslate2](https://github.com/OpenNMT/CTranslate2), a fast inference engine. Check out an [example](https://gist.github.com/ymoslem/60e1d1dc44fe006f67e130b6ad703c4b).\n\n* **Work on low-resource languages:** Find out more details about [how to train NMT models for low-resource languages](https://blog.machinetranslation.io/low-resource-nmt/).\n\n* **Train a multilingual model:** Find out helpful notes about [training multilingual models](https://blog.machinetranslation.io/multilingual-nmt).\n\n* **Publish a demo:** Show off your work through a [simple demo with CTranslate2 and Streamlit](https://blog.machinetranslation.io/nmt-web-interface/).\n","metadata":{"id":"IBi1PhRv4bX9"}}]}