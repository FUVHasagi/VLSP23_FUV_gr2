{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"/home/group2/naTtahN_T2/Tan3010/211023_NhatTan_Test\")\n",
    "sys.path.append(\"..\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:10.679349400Z",
     "start_time": "2023-11-01T19:09:10.661571700Z"
    }
   },
   "id": "5b18ea81271b22ac"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:19.596349500Z",
     "start_time": "2023-11-01T19:09:10.692399400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/group2/naTtahN_T2/Tan3010/211023_NhatTan_Test\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from utilFuncs.modelUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# dataset = load_dataset('alt')\n",
    "# train_dataset = dataset['train']\n",
    "# test_dataset = dataset['test']\n",
    "# train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:19.611867100Z",
     "start_time": "2023-11-01T19:09:19.596349500Z"
    }
   },
   "id": "723dd225dd754d86"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 paths\n"
     ]
    }
   ],
   "source": [
    "filesTrain = ['split_train2023_0to10k.vi', 'split_train2023_0to10k.lo', \n",
    "              'split_train2023_10kto30k.vi', 'split_train2023_10kto30k.lo',\n",
    "              'split_train2023_40kto100k.vi', 'split_train2023_40kto100k.lo']\n",
    "allPathsTrain = findFiles(filesTrain, '../**')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:19.627546300Z",
     "start_time": "2023-11-01T19:09:19.627546300Z"
    }
   },
   "id": "7273609bf0ec6c8"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 6 files\n"
     ]
    }
   ],
   "source": [
    "allDataTrain = readAllData(allPathsTrain)\n",
    "train_dataset = createDataset([allDataTrain[0], allDataTrain[2], allDataTrain[4]],\n",
    "                              [allDataTrain[1], allDataTrain[3], allDataTrain[5]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:33.915822400Z",
     "start_time": "2023-11-01T19:09:19.643228300Z"
    }
   },
   "id": "54cbb8e9c932d694"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 paths\n"
     ]
    }
   ],
   "source": [
    "filesEval = ['split_val2023_0to10k.vi', 'split_val2023_0to10k.lo', \n",
    "              'split_val2023_10kto30k.vi', 'split_val2023_10kto30k.lo',\n",
    "              'split_val2023_40kto100k.vi', 'split_val2023_40kto100k.lo']\n",
    "allPathsEval = findFiles(filesEval, '../**')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:33.962517200Z",
     "start_time": "2023-11-01T19:09:33.961471500Z"
    }
   },
   "id": "d3c3501e392ea152"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 6 files\n"
     ]
    }
   ],
   "source": [
    "allDataEval = readAllData(allPathsEval)\n",
    "eval_dataset = createDataset([allDataEval[0], allDataEval[2], allDataEval[4]],\n",
    "                             [allDataEval[1], allDataEval[3], allDataEval[5]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:36.435125600Z",
     "start_time": "2023-11-01T19:09:33.962517200Z"
    }
   },
   "id": "c9e2cf75f4d44f6d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "TOKENIZER_REPO = 'facebook/bart-base'\n",
    "MODEL_REPO = TOKENIZER_REPO\n",
    "\n",
    "tokenizer, model = loadTokenizerAndSeq2SeqLM(TOKENIZER_REPO, MODEL_REPO,\n",
    "                                             use_pretrained = False)\n",
    "\n",
    "MAX_SEQ_LEN = 144\n",
    "set_tokenizer_lang = False\n",
    "add_lang_token = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:43.042008200Z",
     "start_time": "2023-11-01T19:09:36.435125600Z"
    }
   },
   "id": "441967e2d6081973"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model.config.max_length = MAX_SEQ_LEN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:43.073200100Z",
     "start_time": "2023-11-01T19:09:43.057577900Z"
    }
   },
   "id": "d17c87dbf40305de"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 41552, 15483, 30529,  4082, 30529, 15483, 15698, 34918,  5563,\n",
      "          282,   385,  1376,  2023, 13859,   611, 12369,  1376,  3070,  9470,\n",
      "         2590,   226,  5269,   139,    35,   740,  3695,   257,   295,  5269,\n",
      "          219,   579,  1376,  3070, 10809,  4236,  3602,  8188,  7487,  1376,\n",
      "         2023,  2469,   438,   385,  1376,  2023, 13859,   611,  3553,  5269,\n",
      "          282,   298, 12369,  1376,  3070,  9470,  2590,   226,  5269,   139,\n",
      "            4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1], device='cuda:0')\n",
      "['<s>', '<', '|', '__', 'lo', '__', '|', '>', 'ĠPhi', 'Ãª', 'n', 'Ġd', 'á', '»', 'ĭ', 'ch', 'Ġti', 'á', 'º', '¿', 'ng', 'ĠL', 'Ãł', 'o', ':', 'Ġc', 'Ã¢', 'u', 'Ġn', 'Ãł', 'y', 'Ġs', 'á', 'º', '½', 'ĠÄ', 'ĳ', 'Æ', '°', 'á', '»', '£', 'c', 'Ġd', 'á', '»', 'ĭ', 'ch', 'Ġth', 'Ãł', 'n', 'h', 'Ġti', 'á', 'º', '¿', 'ng', 'ĠL', 'Ãł', 'o', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "<s><|__lo__|> Phiên dịch tiếng Lào: câu này sẽ được dịch thành tiếng Lào.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/group2/miniconda3/envs/pytorch_Tan3010/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     0, 19857, 19857, 19857,  5296,  5296,  5296, 23734, 23734,\n",
      "         23734, 18193, 18193, 18193, 37159, 37159, 37159, 32268, 32268, 32268,\n",
      "          6527,  6527,  6527, 44155, 44155, 44155, 49545, 49545, 49545, 32920,\n",
      "         32920, 32920,    72,    72,    72, 38125, 38125, 38125, 17847, 17847,\n",
      "         17847,  3985,  3985,  3985,  8751,  8751,  8751, 14700, 14700, 14700,\n",
      "         16964, 16964, 16964, 12497, 12497, 12497, 15747, 15747, 15747, 44635,\n",
      "         44635, 44635, 22105, 22105, 22105, 48931, 48931, 48931, 36287, 36287,\n",
      "         36287, 19780, 19780, 19780, 34575, 34575, 34575, 40170, 40170, 40170,\n",
      "         28923, 28923, 28923, 47006, 47006, 47006, 43413, 43413, 43413,  3876,\n",
      "          3876,  3876, 19299, 19299, 19299, 39673, 39673, 39673, 39945, 39945,\n",
      "         39945, 28553, 28553, 28553, 49859, 49859, 49859, 38025, 38025, 38025,\n",
      "         12196, 12196, 12196,  8256,  8256,  8256, 10382, 10382, 10382, 34243,\n",
      "         34243, 34243,  4885,  4885,  4885, 19842, 19842, 19842, 41371, 41371,\n",
      "         41371, 22270, 22270, 22270, 24851, 24851, 24851, 14656, 14656, 14656,\n",
      "         16401, 16401, 16401,  1462,     2]], device='cuda:0')\n",
      "['</s>', '<s>', 'Ġdefines', 'Ġdefines', 'Ġdefines', 'Ġclothing', 'Ġclothing', 'Ġclothing', 'Ġcoats', 'Ġcoats', 'Ġcoats', 'ĠRegulatory', 'ĠRegulatory', 'ĠRegulatory', 'ĠDDR', 'ĠDDR', 'ĠDDR', 'angler', 'angler', 'angler', 'zo', 'zo', 'zo', 'zilla', 'zilla', 'zilla', 'Ġchalleng', 'Ġchalleng', 'Ġchalleng', 'Ġshampoo', 'Ġshampoo', 'Ġshampoo', '.\"', '.\"', '.\"', 'Ġtwe', 'Ġtwe', 'Ġtwe', 'Ġleasing', 'Ġleasing', 'Ġleasing', 'ros', 'ros', 'ros', 'Ġgate', 'Ġgate', 'Ġgate', 'Ġessay', 'Ġessay', 'Ġessay', 'Ġalgorithms', 'Ġalgorithms', 'Ġalgorithms', 'Ġ1985', 'Ġ1985', 'Ġ1985', 'Ġvar', 'Ġvar', 'Ġvar', 'Ġphoton', 'Ġphoton', 'Ġphoton', 'ĠBaron', 'ĠBaron', 'ĠBaron', 'Topic', 'Topic', 'Topic', 'Ġpir', 'Ġpir', 'Ġpir', 'oser', 'oser', 'oser', 'Ġdece', 'Ġdece', 'Ġdece', 'Knowing', 'Knowing', 'Knowing', 'vict', 'vict', 'vict', '();', '();', '();', 'Â·', 'Â·', 'Â·', 'ose', 'ose', 'ose', 'Ġplead', 'Ġplead', 'Ġplead', 'NPR', 'NPR', 'NPR', 'Ġrud', 'Ġrud', 'Ġrud', 'Ġ\"âĢ¦', 'Ġ\"âĢ¦', 'Ġ\"âĢ¦', '||||', '||||', '||||', 'Ġbarbaric', 'Ġbarbaric', 'Ġbarbaric', 'what', 'what', 'what', 'nik', 'nik', 'nik', 'ried', 'ried', 'ried', '840', '840', '840', 'Ġref', 'Ġref', 'Ġref', 'Jo', 'Jo', 'Jo', 'ĠCNS', 'ĠCNS', 'ĠCNS', 'scenes', 'scenes', 'scenes', 'ĠDamien', 'ĠDamien', 'ĠDamien', 'would', 'would', 'would', 'ĠWesley', 'ĠWesley', 'ĠWesley', 'Ġdead', '</s>']\n",
      "</s><s> defines defines defines clothing clothing clothing coats coats coats Regulatory Regulatory Regulatory DDR DDR DDRanglerangleranglerzozozozillazillazilla challeng challeng challeng shampoo shampoo shampoo.\".\".\" twe twe twe leasing leasing leasingrosrosros gate gate gate essay essay essay algorithms algorithms algorithms 1985 1985 1985 var var var photon photon photon Baron Baron BaronTopicTopicTopic pir pir piroseroseroser dece dece deceKnowingKnowingKnowingvictvictvict();();();···oseoseose plead plead pleadNPRNPRNPR rud rud rud \"… \"… \"…|||||||||||| barbaric barbaric barbaricwhatwhatwhatnikniknikriedriedried840840840 ref ref refJoJoJo CNS CNS CNSscenesscenesscenes Damien Damien Damienwouldwouldwould Wesley Wesley Wesley dead</s>\n"
     ]
    }
   ],
   "source": [
    "sampleInputSentence = 'Phiên dịch tiếng Lào: câu này sẽ được dịch thành tiếng Lào.'\n",
    "sampleOutputSentence = 'ການແປພາສາລາວ: ປະໂຫຍກນີ້ຈະຖືກແປເປັນພາສາລາວ.'\n",
    "toyDataset = createDataset([[sampleInputSentence]], [])\n",
    "\n",
    "tokenizerOutput = format_translation_data(toyDataset['translation'][0],\n",
    "                                          tokenizer, set_tokenizer_lang, add_lang_token, \n",
    "                                          input_lang = 'vi', target_lang = 'lo', random_lang = False,\n",
    "                                          MAX_SEQ_LEN = MAX_SEQ_LEN,\n",
    "                                          )\n",
    "print(tokenizerOutput[0])\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizerOutput[0]))\n",
    "print(tokenizer.decode(tokenizerOutput[0]))\n",
    "\n",
    "model.eval()\n",
    "modelOutput = model(tokenizerOutput[0].unsqueeze(0),\n",
    "                    attention_mask = tokenizerOutput[2].unsqueeze(0),\n",
    "                    labels = tokenizerOutput[1].unsqueeze(0))\n",
    "\n",
    "modelGenerate = model.generate(tokenizerOutput[0].unsqueeze(0), max_new_tokens = MAX_SEQ_LEN)\n",
    "print(modelGenerate)\n",
    "\n",
    "output_text = tokenizer.decode(modelGenerate[0])\n",
    "print(tokenizer.convert_ids_to_tokens(modelGenerate[0]))\n",
    "print(output_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:44.896268400Z",
     "start_time": "2023-11-01T19:09:43.088827Z"
    }
   },
   "id": "d519aa403c54ec15"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "add_lang_tokens_to_tokenizer(tokenizer, model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:45.193067300Z",
     "start_time": "2023-11-01T19:09:44.896268400Z"
    }
   },
   "id": "7637cd233eb41a8a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<|__lo__|> ', 'Ph', 'i', 'Ãª', 'n', 'Ġd', 'á', '»', 'ĭ', 'ch', 'Ġti', 'á', 'º', '¿', 'ng', 'ĠL', 'Ãł', 'o', ':', 'Ġc', 'Ã¢', 'u', 'Ġn', 'Ãł', 'y', 'Ġs', 'á', 'º', '½', 'ĠÄ', 'ĳ', 'Æ', '°', 'á', '»', '£', 'c', 'Ġd', 'á', '»', 'ĭ', 'ch', 'Ġth', 'Ãł', 'n', 'h', 'Ġti', 'á', 'º', '¿', 'ng', 'ĠL', 'Ãł', 'o', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "tokenizerOutput = format_translation_data(toyDataset['translation'][0],\n",
    "                                          tokenizer, set_tokenizer_lang, add_lang_token, \n",
    "                                          input_lang = 'vi', target_lang = 'lo', random_lang = False,\n",
    "                                          MAX_SEQ_LEN = MAX_SEQ_LEN,\n",
    "                                          )\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizerOutput[0])\n",
    "print(tokens) # Make sure that the special translation token is not 'fragmented'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:45.318063400Z",
     "start_time": "2023-11-01T19:09:45.208742400Z"
    }
   },
   "id": "35c1ef7eda4f8b24"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|__lo__|>  Ph i Ãª n Ġd á » ĭ ch Ġti á º ¿ ng ĠL Ãł o : Ġc Ã¢ u Ġn Ãł y Ġs á º ½ ĠÄ ĳ Æ ° á » £ c Ġd á » ĭ ch Ġth Ãł n h Ġti á º ¿ ng ĠL Ãł o . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<s> à º ģ à º ² à º Ļ à » ģ à º Ľ à º ŀ à º ² à º ª à º ² à º ¥ à º ² à º § : Ġ à º Ľ à º ° à » Ĥ à º « à º į à º ģ à º Ļ à º µ à » ī à º Ī à º ° à º ĸ à º · à º ģ à » ģ à º Ľ à » Ģ à º Ľ à º ± à º Ļ à º ŀ à º ² à º ª à º ² à º ¥ à º ² à º § . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Input shape: torch.Size([16, 144])\n",
      "Output shape: torch.Size([16, 144])\n",
      "Attention mask shape: torch.Size([16, 144])\n"
     ]
    }
   ],
   "source": [
    "# Testing `data_transform`\n",
    "toyDataset = createDataset([[sampleInputSentence]], [[sampleOutputSentence]])\n",
    "in_ids, out_ids, attention_mask = format_translation_data(toyDataset['translation'][0],\n",
    "                                                          tokenizer, set_tokenizer_lang, add_lang_token, \n",
    "                                                          input_lang = 'vi', target_lang = 'lo', random_lang = False,\n",
    "                                                          MAX_SEQ_LEN = MAX_SEQ_LEN,\n",
    "                                                          )\n",
    "\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(in_ids)))\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(out_ids)))\n",
    "\n",
    "# Testing data generator\n",
    "data_gen = get_data_generator(\n",
    "    train_dataset,\n",
    "    tokenizer, set_tokenizer_lang, add_lang_token,\n",
    "    MAX_SEQ_LEN,\n",
    "    input_lang = 'vi', target_lang = 'lo', random_lang = False,\n",
    ")\n",
    "data_batch = next(data_gen)\n",
    "print('Input shape:', data_batch[0].shape)\n",
    "print('Output shape:', data_batch[1].shape)\n",
    "print('Attention mask shape:', data_batch[2].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:45.364944Z",
     "start_time": "2023-11-01T19:09:45.318063400Z"
    }
   },
   "id": "fdd180a81834c96b"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Constants\n",
    "n_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "print_freq = 100\n",
    "checkpoint_freq = 500\n",
    "\n",
    "lr = 7.5e-4\n",
    "\n",
    "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
    "total_steps = n_epochs * n_batches\n",
    "n_warmup_steps = int(total_steps * 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:45.380726500Z",
     "start_time": "2023-11-01T19:09:45.364944Z"
    }
   },
   "id": "2ba34ecee3fe2ab"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/group2/miniconda3/envs/pytorch_Tan3010/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, n_warmup_steps, total_steps\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:45.536981Z",
     "start_time": "2023-11-01T19:09:45.396186700Z"
    }
   },
   "id": "171f2f4cff6df0db"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model_name = 'bartBase'\n",
    "n_direction = 2\n",
    "data_description = 'AllCleaned'\n",
    "model_checkpoint = ('%s_%dD_%s_batch%d_checkpoint.pt' \n",
    "                    % (model_name, n_direction, data_description, batch_size))\n",
    "model_path = ('%s_%dD_%s_batch%d_fineTunedEpoch{}.pt' \n",
    "                    % (model_name, n_direction, data_description, batch_size))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:45.552441100Z",
     "start_time": "2023-11-01T19:09:45.536981Z"
    }
   },
   "id": "fbdd8d3436d84522"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "input_lang = None\n",
    "target_lang = None\n",
    "random_lang = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:09:45.552441100Z",
     "start_time": "2023-11-01T19:09:45.536981Z"
    }
   },
   "id": "f97a8a3f597efd41"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2391 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44c177904a504f9fb9175f4272727d52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m losses, evalLosses \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_tokenizer_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_lang_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mMAX_SEQ_LEN\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                           \u001B[49m\u001B[43minput_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_lang\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                           \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mprint_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheckpoint_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mmodel_checkpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/naTtahN_T2/Tan3010/211023_NhatTan_Test/utilFuncs/modelUtils.py:190\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, tokenizer, set_tokenizer_lang, add_lang_token, MAX_SEQ_LEN, input_lang, target_lang, random_lang, batch_size, optimizer, scheduler, n_epochs, n_batches, train_dataset, eval_dataset, print_freq, checkpoint_freq, model_checkpoint, model_path)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_epochs):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Randomize data order\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     data_generator \u001B[38;5;241m=\u001B[39m get_data_generator(train_dataset,\n\u001B[1;32m    185\u001B[0m                                         tokenizer, set_tokenizer_lang, add_lang_token,\n\u001B[1;32m    186\u001B[0m                                         MAX_SEQ_LEN,\n\u001B[1;32m    187\u001B[0m                                         input_lang, target_lang, random_lang,\n\u001B[1;32m    188\u001B[0m                                         batch_size \u001B[38;5;241m=\u001B[39m batch_size)\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (input_batch, label_batch, attention_mask_batch) \\\n\u001B[1;32m    191\u001B[0m             \u001B[38;5;129;01min\u001B[39;00m tqdm_notebook(\u001B[38;5;28menumerate\u001B[39m(data_generator), total\u001B[38;5;241m=\u001B[39mn_batches):\n\u001B[1;32m    193\u001B[0m         model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m    194\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch_Tan3010/lib/python3.9/site-packages/tqdm/notebook.py:249\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    248\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(tqdm_notebook, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m    252\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch_Tan3010/lib/python3.9/site-packages/tqdm/std.py:1182\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1179\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1182\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1185\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/naTtahN_T2/Tan3010/211023_NhatTan_Test/utilFuncs/modelUtils.py:122\u001B[0m, in \u001B[0;36mget_data_generator\u001B[0;34m(dataset, tokenizer, set_tokenizer_lang, add_lang_token, MAX_SEQ_LEN, input_lang, target_lang, random_lang, batch_size, shuffle)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(dataset), batch_size):\n\u001B[1;32m    121\u001B[0m     raw_batch \u001B[38;5;241m=\u001B[39m dataset[i : i \u001B[38;5;241m+\u001B[39m batch_size]\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[43mtransform_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_tokenizer_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_lang_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mMAX_SEQ_LEN\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m                          \u001B[49m\u001B[43minput_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_lang\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/naTtahN_T2/Tan3010/211023_NhatTan_Test/utilFuncs/modelUtils.py:84\u001B[0m, in \u001B[0;36mtransform_batch\u001B[0;34m(batch, tokenizer, set_tokenizer_lang, add_lang_token, MAX_SEQ_LEN, input_lang, target_lang, random_lang, DEVICE)\u001B[0m\n\u001B[1;32m     81\u001B[0m attentionMask \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m translation_set \u001B[38;5;129;01min\u001B[39;00m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtranslation\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m---> 84\u001B[0m     formatted_data \u001B[38;5;241m=\u001B[39m \u001B[43mformat_translation_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtranslation_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_tokenizer_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_lang_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m        \u001B[49m\u001B[43mMAX_SEQ_LEN\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_lang\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (formatted_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     92\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[0;32m~/naTtahN_T2/Tan3010/211023_NhatTan_Test/utilFuncs/modelUtils.py:64\u001B[0m, in \u001B[0;36mformat_translation_data\u001B[0;34m(translations, tokenizer, set_tokenizer_lang, add_lang_token, MAX_SEQ_LEN, input_lang, target_lang, random_lang, LANGS)\u001B[0m\n\u001B[1;32m     61\u001B[0m     tokenizer\u001B[38;5;241m.\u001B[39msrc_lang \u001B[38;5;241m=\u001B[39m input_lang\n\u001B[1;32m     62\u001B[0m     tokenizer\u001B[38;5;241m.\u001B[39mtgt_lang \u001B[38;5;241m=\u001B[39m target_lang\n\u001B[0;32m---> 64\u001B[0m input_token_ids, target_token_ids, attention_mask \u001B[38;5;241m=\u001B[39m \u001B[43mencode_str\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_lang_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_lang\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m    \u001B[49m\u001B[43mMAX_SEQ_LEN\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m input_token_ids, target_token_ids, attention_mask\n",
      "File \u001B[0;32m~/naTtahN_T2/Tan3010/211023_NhatTan_Test/utilFuncs/modelUtils.py:29\u001B[0m, in \u001B[0;36mencode_str\u001B[0;34m(text, text_target, tokenizer, add_lang_token, target_lang, MAX_SEQ_LEN, LANG_TOKEN_MAPPING, DEVICE)\u001B[0m\n\u001B[1;32m     26\u001B[0m     target_lang_token \u001B[38;5;241m=\u001B[39m LANG_TOKEN_MAPPING[target_lang]\n\u001B[1;32m     27\u001B[0m     text \u001B[38;5;241m=\u001B[39m target_lang_token \u001B[38;5;241m+\u001B[39m text\n\u001B[0;32m---> 29\u001B[0m tokenizerOutp \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_target\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_target\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax_length\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMAX_SEQ_LEN\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (tokenizerOutp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m], tokenizerOutp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m], tokenizerOutp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch_Tan3010/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2541\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2540\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n\u001B[0;32m-> 2541\u001B[0m     target_encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2542\u001B[0m \u001B[38;5;66;03m# Leave back tokenizer in input mode\u001B[39;00m\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch_Tan3010/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2644\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2624\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_encode_plus(\n\u001B[1;32m   2625\u001B[0m         batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[1;32m   2626\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2641\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2642\u001B[0m     )\n\u001B[1;32m   2643\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2644\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2645\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2646\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2647\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2648\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2649\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2650\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2651\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2652\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2653\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2654\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2655\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2656\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2657\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2658\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2659\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2660\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2661\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2662\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2663\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch_Tan3010/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2708\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2687\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2688\u001B[0m \u001B[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001B[39;00m\n\u001B[1;32m   2689\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2704\u001B[0m \u001B[38;5;124;03m        method).\u001B[39;00m\n\u001B[1;32m   2705\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2707\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[0;32m-> 2708\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_padding_truncation_strategies\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2711\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2712\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2714\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2715\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2717\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encode_plus(\n\u001B[1;32m   2718\u001B[0m     text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   2719\u001B[0m     text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2735\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2736\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch_Tan3010/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2442\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001B[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2439\u001B[0m             max_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_max_length\n\u001B[1;32m   2441\u001B[0m \u001B[38;5;66;03m# Test if we have a padding token\u001B[39;00m\n\u001B[0;32m-> 2442\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m padding_strategy \u001B[38;5;241m!=\u001B[39m PaddingStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_PAD \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m   2443\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2444\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2445\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2446\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpad_token\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[PAD]\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m})`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2447\u001B[0m     )\n\u001B[1;32m   2449\u001B[0m \u001B[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "losses, evalLosses = train(model,\n",
    "                           tokenizer, set_tokenizer_lang, add_lang_token,\n",
    "                           MAX_SEQ_LEN,\n",
    "                           input_lang, target_lang, random_lang,\n",
    "                           batch_size,\n",
    "                           optimizer, scheduler,\n",
    "                           n_epochs, n_batches,\n",
    "                           train_dataset, eval_dataset,\n",
    "                           print_freq, checkpoint_freq,\n",
    "                           model_checkpoint, model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:10:27.481499700Z",
     "start_time": "2023-11-01T19:09:45.536981Z"
    }
   },
   "id": "460f2188a27c38b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotLoss(losses, smoothing_window_size = 50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T19:10:27.470838200Z"
    }
   },
   "id": "8c269e3ea7affc59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotLoss(evalLosses, smoothing_window_size = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T19:10:27.470838200Z"
    }
   },
   "id": "8515475f243d0b01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_model(model, eval_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:10:27.481920700Z",
     "start_time": "2023-11-01T19:10:27.481920700Z"
    }
   },
   "id": "c151cf333d8a1bf1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
