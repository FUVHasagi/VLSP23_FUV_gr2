{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D5b6NUKdP6C0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5b6NUKdP6C0",
    "outputId": "a205e229-44ed-44c5-9d0e-6f5e00c97a70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ev_HRE8WQCgT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ev_HRE8WQCgT",
    "outputId": "56bf31e0-42a4-402c-9f7a-50f38c9c8515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets sentencepiece transformers evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "initial_id",
    "is_executing": true,
    "outputId": "789ac8ce-e400-4d08-aaf7-afaf150b9c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm_notebook\n",
    "import evaluate\n",
    "\n",
    "sns.set()\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device: %s\" % (device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd26e780aa9d3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9bd26e780aa9d3d",
    "outputId": "23681d48-2f53-4860-d423-e5a3393a32a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SNT.URLID': '80188',\n",
       " 'SNT.URLID.SNTID': '1',\n",
       " 'url': 'http://en.wikinews.org/wiki/2007_Rugby_World_Cup:_Italy_31_-_5_Portugal',\n",
       " 'translation': {'bg': 'ফ্রান্সের প্যারিসের পার্ক দি প্রিন্সেস-এ হওয়া ২০০৭-এর রাগবি বিশ্বকাপের পুল সি-তে ইটালি পর্তুগালকে ৩১-৫ গোলে হারিয়েছে।',\n",
       "  'en': 'Italy have defeated Portugal 31-5 in Pool C of the 2007 Rugby World Cup at Parc des Princes, Paris, France.',\n",
       "  'en_tok': 'Italy have defeated Portugal 31-5 in Pool C of the 2007 Rugby World Cup at Parc des Princes , Paris , France .',\n",
       "  'fil': 'Natalo ng Italya ang Portugal sa puntos na 31-5 sa Grupong C noong 2007 sa Pandaigdigang laro ng Ragbi sa Parc des Princes, Paris, France.',\n",
       "  'hi': '2007 में फ़्रांस, पेरिस के पार्क डेस प्रिंसेस में हुए रग्बी विश्व कप के पूल C में इटली ने पुर्तगाल को 31-5 से हराया।',\n",
       "  'id': 'Italia berhasil mengalahkan Portugal 31-5 di grup C dalam Piala Dunia Rugby 2007 di Parc des Princes, Paris, Perancis.',\n",
       "  'ja': 'フランスのパリ、パルク・デ・プランスで行われた2007年ラグビーワールドカップのプールCで、イタリアは31対5でポルトガルを下した。',\n",
       "  'khm': 'អ៊ីតាលីបានឈ្នះលើព័រទុយហ្គាល់ 31-5 ក្នុងប៉ូលCនៃពីធីប្រកួតពានរង្វាន់ពិភពលោកនៃកីឡាបាល់ឱបឆ្នាំ2007ដែលប្រព្រឹត្តនៅប៉ាសឌេសប្រីន ក្រុងប៉ារីស បារាំង។',\n",
       "  'lo': 'ອິຕາລີໄດ້ເສຍໃຫ້ປ໊ອກຕຸຍການ 31 ຕໍ່ 5 ໃນພູລ C ຂອງ ການແຂ່ງຂັນຣັກບີ້ລະດັບໂລກປີ 2007 ທີ່ ປາກເດແພຣັງ ປາຣີ ປະເທດຝຣັ່ງ.',\n",
       "  'ms': 'Itali telah mengalahkan Portugal 31-5 dalam Pool C pada Piala Dunia Ragbi 2007 di Parc des Princes, Paris, Perancis.',\n",
       "  'my': 'ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂၀၀၇ခုနှစ် ရပ်ဘီ ကမ္ဘာ့ ဖလား တွင် အီတလီ သည် ပေါ်တူဂီ ကို ၃၁-၅ ဂိုး ဖြင့် ရေကူးကန် စီ တွင် ရှုံးနိမ့်သွားပါသည် ။',\n",
       "  'th': 'อิตาลีได้เอาชนะโปรตุเกสด้วยคะแนน31ต่อ5 ในกลุ่มc ของการแข่งขันรักบี้เวิลด์คัพปี2007 ที่สนามปาร์กเดแพร็งส์ ที่กรุงปารีส ประเทศฝรั่งเศส',\n",
       "  'vi': 'Ý đã đánh bại Bồ Đào Nha với tỉ số 31-5 ở Bảng C Giải vô địch Rugby thế giới 2007 tại Parc des Princes, Pari, Pháp.',\n",
       "  'zh': '意大利在法国巴黎王子公园体育场举办的2007年橄榄球世界杯C组以31-5击败葡萄牙。'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('alt')\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98a9e97984f82f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T06:59:35.251344500Z",
     "start_time": "2023-10-23T06:59:28.422397300Z"
    },
    "id": "ba98a9e97984f82f"
   },
   "outputs": [],
   "source": [
    "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8891c86803f764",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T06:59:47.874087500Z",
     "start_time": "2023-10-23T06:59:43.093828700Z"
    },
    "id": "cb8891c86803f764"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/m2m/m2m100_418M_FineTunedEpoch14_Early.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53be91523320515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T06:59:47.892775100Z",
     "start_time": "2023-10-23T06:59:47.888231900Z"
    },
    "id": "a53be91523320515"
   },
   "outputs": [],
   "source": [
    "max_seq_len = model.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-35Y0W0wQxdF",
   "metadata": {
    "id": "-35Y0W0wQxdF"
   },
   "outputs": [],
   "source": [
    "def encode_str(text, text_target, tokenizer, seq_len):\n",
    "\n",
    "    # Tokenize and add special tokens\n",
    "    tokenizerOutp = tokenizer(\n",
    "        text = text,\n",
    "        text_target = text_target,\n",
    "        return_tensors = 'pt',\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = seq_len).to(device)\n",
    "\n",
    "    return tokenizerOutp['input_ids'][0], tokenizerOutp['labels'][0], tokenizerOutp['attention_mask'][0]\n",
    "\n",
    "\n",
    "def format_translation_data(translations, tokenizer, input_lang, target_lang, seq_len = max_seq_len):\n",
    "\n",
    "    # Get the translations for the batch\n",
    "    input_text = translations[input_lang]\n",
    "    target_text = translations[target_lang]\n",
    "\n",
    "    if input_text is None or target_text is None:\n",
    "        return None\n",
    "\n",
    "    if ((input_lang == 'lo') & (target_lang == 'vi')):\n",
    "        tokenizer.src_lang = \"lo\"\n",
    "        tokenizer.tgt_lang = \"vi\"\n",
    "    elif ((input_lang == 'vi') & (target_lang == 'lo')):\n",
    "        tokenizer.src_lang = \"vi\"\n",
    "        tokenizer.tgt_lang = \"lo\"\n",
    "    else:\n",
    "        print('WARNING: SOMETHING WRONG WHEN RANDOMIZING LANG')\n",
    "\n",
    "    input_token_ids, target_token_ids, attention_mask = encode_str(\n",
    "        input_text, target_text, tokenizer, seq_len)\n",
    "\n",
    "    return input_token_ids, target_token_ids, attention_mask\n",
    "\n",
    "\n",
    "def transform_batch(batch, tokenizer, input_lang, target_lang):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    attentionMask = []\n",
    "    for translation_set in batch['translation']:\n",
    "        formatted_data = format_translation_data(\n",
    "            translation_set, tokenizer, input_lang, target_lang, max_seq_len)\n",
    "\n",
    "        if formatted_data is None:\n",
    "            continue\n",
    "\n",
    "        input_ids, target_ids, attention_mask = formatted_data\n",
    "\n",
    "        inputs.append(input_ids.unsqueeze(0))\n",
    "        targets.append(target_ids.unsqueeze(0))\n",
    "        attentionMask.append(attention_mask.unsqueeze(0))\n",
    "\n",
    "    batch_input_ids = torch.cat(inputs).cuda()\n",
    "    batch_target_ids = torch.cat(targets).cuda()\n",
    "    attentionMask = torch.cat(attentionMask).cuda()\n",
    "\n",
    "    return batch_input_ids, batch_target_ids, attentionMask\n",
    "\n",
    "\n",
    "def get_data_generator(dataset, input_lang, target_lang, tokenizer, batch_size = 32):\n",
    "    dataset = dataset.shuffle()\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        raw_batch = dataset[i:i+batch_size]\n",
    "        yield transform_batch(raw_batch, tokenizer, input_lang, target_lang)\n",
    "\n",
    "\n",
    "def eval_model(model, tokenizer, gdataset, input_lang = 'lo', target_lang = 'vi'):\n",
    "\n",
    "    model.eval()\n",
    "    test_generator = get_data_generator(gdataset, input_lang, target_lang, tokenizer, batch_size = 4)\n",
    "\n",
    "    trueSentenceListLoVi = []\n",
    "    outputSentenceListLoVi = []\n",
    "\n",
    "    trueSentenceListViLo = []\n",
    "    outputSentenceListViLo = []\n",
    "\n",
    "    for i, (input_batch, label_batch, attention_mask_batch) in enumerate(test_generator):\n",
    "\n",
    "        outpSentenceLoVi = model.generate(input_batch, num_beams = 20, num_return_sequences=1, max_new_tokens = max_seq_len, forced_bos_token_id = tokenizer.get_lang_id(target_lang))\n",
    "        outpSentenceLoVi = tokenizer.batch_decode(outpSentenceLoVi, skip_special_tokens = True)\n",
    "        outputSentenceListLoVi = outputSentenceListLoVi + outpSentenceLoVi\n",
    "\n",
    "        trueSentenceLoVi = tokenizer.batch_decode(label_batch, skip_special_tokens = True)\n",
    "        trueSentenceListLoVi = trueSentenceListLoVi + trueSentenceLoVi\n",
    "\n",
    "\n",
    "        outpSentenceViLo = model.generate(label_batch, num_beams = 20, num_return_sequences=1, max_new_tokens = max_seq_len, forced_bos_token_id = tokenizer.get_lang_id(input_lang))\n",
    "        outpSentenceViLo = tokenizer.batch_decode(outpSentenceViLo, skip_special_tokens = True)\n",
    "        outputSentenceListViLo = outputSentenceListViLo + outpSentenceViLo\n",
    "\n",
    "        trueSentenceViLo = tokenizer.batch_decode(input_batch, skip_special_tokens = True)\n",
    "        trueSentenceListViLo = trueSentenceListViLo + trueSentenceViLo\n",
    "\n",
    "\n",
    "    print(len(trueSentenceListLoVi))\n",
    "    print(len(outputSentenceListLoVi))\n",
    "    print(len(trueSentenceListViLo))\n",
    "    print(len(outputSentenceListViLo))\n",
    "\n",
    "    return (trueSentenceListLoVi, outputSentenceListLoVi, trueSentenceListViLo, outputSentenceListViLo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# srcLang = \"lo\"\n",
    "# tgtLang = \"vi\"\n",
    "\n",
    "srcLang = \"vi\"\n",
    "tgtLang = \"lo\"\n",
    "\n",
    "inputText = \"Neil Armstrong là người đầu tiên bước chân lên mặt trăng.\"\n",
    "outputText = \"\"\n",
    "\n",
    "tokenizer.src_lang = srcLang\n",
    "tokenizer.tgt_lang = tgtLang\n",
    "encodedInputs = encode_str(inputText, outputText, tokenizer, max_seq_len)\n",
    "\n",
    "outputSent = model.generate(encodedInputs[0].unsqueeze(0), num_beams = 50, num_return_sequences = 3, max_new_tokens = max_seq_len, forced_bos_token_id = tokenizer.get_lang_id(tgtLang))\n",
    "outputSent = tokenizer.batch_decode(outputSent, skip_special_tokens = True)\n",
    "print(outputSent)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b09da9a338df2212"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coizLw3SYOgT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coizLw3SYOgT",
    "outputId": "87f229a5-2ae0-4d13-d4e5-aba157240af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016\n",
      "1016\n",
      "1016\n",
      "1016\n"
     ]
    }
   ],
   "source": [
    "trueSentenceListLoVi, outputSentenceListLoVi, trueSentenceListViLo, outputSentenceListViLo = eval_model(model, tokenizer, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/content/drive/MyDrive/outputSentenceListLoVi.json\", 'w') as f:\n",
    "    json.dump(outputSentenceListLoVi, f, indent = 2, ensure_ascii = False)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/outputSentenceListViLo.json\", 'w') as f:\n",
    "    json.dump(outputSentenceListViLo, f, indent = 2, ensure_ascii = False)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/trueSentenceListLoVi.json\", 'w') as f:\n",
    "    json.dump(trueSentenceListLoVi, f, indent = 2, ensure_ascii = False)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/trueSentenceListViLo.json\", 'w') as f:\n",
    "    json.dump(trueSentenceListViLo, f, indent = 2, ensure_ascii = False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a96258a1d110a9c"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
