{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5b6NUKdP6C0",
        "outputId": "e02e908b-a384-41d6-c591-8489f6538fc5"
      },
      "id": "D5b6NUKdP6C0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets sentencepiece transformers evaluate sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev_HRE8WQCgT",
        "outputId": "8f1c2508-f96b-4963-8f14-0842db18f971"
      },
      "id": "Ev_HRE8WQCgT",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "is_executing": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "088d0475-b125-41ae-b3d0-629d3c8f8b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm_notebook\n",
        "import evaluate\n",
        "\n",
        "sns.set()\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Using device: %s\" % (device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SNT.URLID': '80188',\n",
              " 'SNT.URLID.SNTID': '1',\n",
              " 'url': 'http://en.wikinews.org/wiki/2007_Rugby_World_Cup:_Italy_31_-_5_Portugal',\n",
              " 'translation': {'bg': 'ফ্রান্সের প্যারিসের পার্ক দি প্রিন্সেস-এ হওয়া ২০০৭-এর রাগবি বিশ্বকাপের পুল সি-তে ইটালি পর্তুগালকে ৩১-৫ গোলে হারিয়েছে।',\n",
              "  'en': 'Italy have defeated Portugal 31-5 in Pool C of the 2007 Rugby World Cup at Parc des Princes, Paris, France.',\n",
              "  'en_tok': 'Italy have defeated Portugal 31-5 in Pool C of the 2007 Rugby World Cup at Parc des Princes , Paris , France .',\n",
              "  'fil': 'Natalo ng Italya ang Portugal sa puntos na 31-5 sa Grupong C noong 2007 sa Pandaigdigang laro ng Ragbi sa Parc des Princes, Paris, France.',\n",
              "  'hi': '2007 में फ़्रांस, पेरिस के पार्क डेस प्रिंसेस में हुए रग्बी विश्व कप के पूल C में इटली ने पुर्तगाल को 31-5 से हराया।',\n",
              "  'id': 'Italia berhasil mengalahkan Portugal 31-5 di grup C dalam Piala Dunia Rugby 2007 di Parc des Princes, Paris, Perancis.',\n",
              "  'ja': 'フランスのパリ、パルク・デ・プランスで行われた2007年ラグビーワールドカップのプールCで、イタリアは31対5でポルトガルを下した。',\n",
              "  'khm': 'អ៊ីតាលីបានឈ្នះលើព័រទុយហ្គាល់ 31-5 ក្នុងប៉ូលCនៃពីធីប្រកួតពានរង្វាន់ពិភពលោកនៃកីឡាបាល់ឱបឆ្នាំ2007ដែលប្រព្រឹត្តនៅប៉ាសឌេសប្រីន ក្រុងប៉ារីស បារាំង។',\n",
              "  'lo': 'ອິຕາລີໄດ້ເສຍໃຫ້ປ໊ອກຕຸຍການ 31 ຕໍ່ 5 ໃນພູລ C ຂອງ ການແຂ່ງຂັນຣັກບີ້ລະດັບໂລກປີ 2007 ທີ່ ປາກເດແພຣັງ ປາຣີ ປະເທດຝຣັ່ງ.',\n",
              "  'ms': 'Itali telah mengalahkan Portugal 31-5 dalam Pool C pada Piala Dunia Ragbi 2007 di Parc des Princes, Paris, Perancis.',\n",
              "  'my': 'ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂၀၀၇ခုနှစ် ရပ်ဘီ ကမ္ဘာ့ ဖလား တွင် အီတလီ သည် ပေါ်တူဂီ ကို ၃၁-၅ ဂိုး ဖြင့် ရေကူးကန် စီ တွင် ရှုံးနိမ့်သွားပါသည် ။',\n",
              "  'th': 'อิตาลีได้เอาชนะโปรตุเกสด้วยคะแนน31ต่อ5 ในกลุ่มc ของการแข่งขันรักบี้เวิลด์คัพปี2007 ที่สนามปาร์กเดแพร็งส์ ที่กรุงปารีส ประเทศฝรั่งเศส',\n",
              "  'vi': 'Ý đã đánh bại Bồ Đào Nha với tỉ số 31-5 ở Bảng C Giải vô địch Rugby thế giới 2007 tại Parc des Princes, Pari, Pháp.',\n",
              "  'zh': '意大利在法国巴黎王子公园体育场举办的2007年橄榄球世界杯C组以31-5击败葡萄牙。'}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dataset = load_dataset('alt')\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9bd26e780aa9d3d",
        "outputId": "7e908ef7-b9c8-4d38-a02f-bed64eea2731"
      },
      "id": "e9bd26e780aa9d3d"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T06:59:35.251344500Z",
          "start_time": "2023-10-23T06:59:28.422397300Z"
        },
        "id": "ba98a9e97984f82f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63741a0-0860-44a3-f2bc-ce66df76f191"
      },
      "id": "ba98a9e97984f82f"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "max_seq_len = 128\n",
        "\n",
        "# LANG_TOKEN_MAPPING = {\n",
        "#     'vi': '<vi>',\n",
        "#     'lo': '<lo>',\n",
        "# }\n",
        "\n",
        "# LANG_TOKEN_MAPPING = {\n",
        "#     'vi': 'translate Lao to Vietnamese: ',\n",
        "#     'lo': 'translate Vietnamese to Lao: ',\n",
        "# }\n",
        "\n",
        "LANG_TOKEN_MAPPING = {\n",
        "    'vi': '<vi>',\n",
        "    'lo': '<lao>'\n",
        "}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T06:59:47.892775100Z",
          "start_time": "2023-10-23T06:59:47.888231900Z"
        },
        "id": "a53be91523320515"
      },
      "id": "a53be91523320515"
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/mt5_translation.pt\"))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44bDEng_4cS_",
        "outputId": "b63a8193-0403-4a73-9c40-4186b5477e6b"
      },
      "id": "44bDEng_4cS_",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MT5ForConditionalGeneration(\n",
              "  (shared): Embedding(250102, 768)\n",
              "  (encoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250102, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250102, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=250102, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_str(text, text_target, tokenizer, seq_len, lang_token_map=LANG_TOKEN_MAPPING):\n",
        "\n",
        "    input_lang_token = lang_token_map['lo']\n",
        "    target_lang_token = lang_token_map['vi']\n",
        "\n",
        "    # Tokenize and add special tokens\n",
        "    tokenizerOutp = tokenizer(\n",
        "        text = target_lang_token + text,\n",
        "        text_target = text_target,\n",
        "        return_tensors = 'pt',\n",
        "        padding = 'max_length',\n",
        "        truncation = True,\n",
        "        max_length = seq_len).to(device)\n",
        "\n",
        "    tokenizerOutp1 = tokenizer(\n",
        "        text = input_lang_token + text_target,\n",
        "        text_target = text,\n",
        "        return_tensors = 'pt',\n",
        "        padding = 'max_length',\n",
        "        truncation = True,\n",
        "        max_length = seq_len).to(device)\n",
        "\n",
        "    return tokenizerOutp['input_ids'][0], tokenizerOutp1['input_ids'][0], tokenizerOutp['attention_mask'][0] # Lo, Vi, AttenM\n",
        "\n",
        "\n",
        "def format_translation_data(translations, tokenizer, input_lang = 'lo', target_lang = 'vi', seq_len = max_seq_len):\n",
        "\n",
        "    # Get the translations for the batch\n",
        "    input_text = translations[input_lang]\n",
        "    target_text = translations[target_lang]\n",
        "\n",
        "    if input_text is None or target_text is None:\n",
        "        return None\n",
        "\n",
        "    input_token_ids, target_token_ids, attention_mask = encode_str(\n",
        "        input_text, target_text, tokenizer, seq_len)\n",
        "\n",
        "    return input_token_ids, target_token_ids, attention_mask\n",
        "\n",
        "\n",
        "def transform_batch(batch, tokenizer, input_lang, target_lang):\n",
        "\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    attentionMask = []\n",
        "\n",
        "    for translation_set in batch['translation']:\n",
        "        formatted_data = format_translation_data(\n",
        "            translation_set, tokenizer, input_lang, target_lang, max_seq_len)\n",
        "\n",
        "        if formatted_data is None:\n",
        "            continue\n",
        "\n",
        "        input_ids, target_ids, attention_mask = formatted_data\n",
        "\n",
        "        inputs.append(input_ids.unsqueeze(0))\n",
        "        targets.append(target_ids.unsqueeze(0))\n",
        "        attentionMask.append(attention_mask.unsqueeze(0))\n",
        "\n",
        "    batch_input_ids = torch.cat(inputs).cuda()\n",
        "    batch_target_ids = torch.cat(targets).cuda()\n",
        "    attentionMask = torch.cat(attentionMask).cuda()\n",
        "\n",
        "    return batch_input_ids, batch_target_ids, attentionMask\n",
        "\n",
        "\n",
        "def get_data_generator(dataset, input_lang, target_lang, tokenizer, batch_size = 32):\n",
        "    dataset = dataset.shuffle()\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        raw_batch = dataset[i:i+batch_size]\n",
        "        yield transform_batch(raw_batch, tokenizer, input_lang, target_lang)\n",
        "\n",
        "\n",
        "def eval_model(model, tokenizer, gdataset, input_lang = 'lo', target_lang = 'vi'):\n",
        "\n",
        "    model.eval()\n",
        "    test_generator = get_data_generator(gdataset, input_lang, target_lang, tokenizer, batch_size = 6)\n",
        "\n",
        "    trueSentenceListLoVi = []\n",
        "    outputSentenceListLoVi = []\n",
        "\n",
        "    trueSentenceListViLo = []\n",
        "    outputSentenceListViLo = []\n",
        "\n",
        "    for i, (input_batch, label_batch, attention_mask_batch) in enumerate(test_generator):\n",
        "\n",
        "        outpSentenceLoVi = model.generate(input_batch, num_beams = 20, num_return_sequences=1, max_new_tokens = max_seq_len)\n",
        "        outpSentenceLoVi = tokenizer.batch_decode(outpSentenceLoVi, skip_special_tokens = True)\n",
        "        outputSentenceListLoVi = outputSentenceListLoVi + outpSentenceLoVi\n",
        "\n",
        "        trueSentenceLoVi = tokenizer.batch_decode(label_batch, skip_special_tokens = True)\n",
        "        trueSentenceListLoVi = trueSentenceListLoVi + trueSentenceLoVi\n",
        "\n",
        "\n",
        "        outpSentenceViLo = model.generate(label_batch, num_beams = 20, num_return_sequences=1, max_new_tokens = max_seq_len)\n",
        "        outpSentenceViLo = tokenizer.batch_decode(outpSentenceViLo, skip_special_tokens = True)\n",
        "        outputSentenceListViLo = outputSentenceListViLo + outpSentenceViLo\n",
        "\n",
        "        trueSentenceViLo = tokenizer.batch_decode(input_batch, skip_special_tokens = True)\n",
        "        trueSentenceListViLo = trueSentenceListViLo + trueSentenceViLo\n",
        "\n",
        "\n",
        "    print(len(trueSentenceListLoVi))\n",
        "    print(len(outputSentenceListLoVi))\n",
        "    print(len(trueSentenceListViLo))\n",
        "    print(len(outputSentenceListViLo))\n",
        "\n",
        "    return (trueSentenceListLoVi, outputSentenceListLoVi, trueSentenceListViLo, outputSentenceListViLo)\n"
      ],
      "metadata": {
        "id": "-35Y0W0wQxdF"
      },
      "id": "-35Y0W0wQxdF",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputText = \"\"\n",
        "outputText = \"Đã có thông tin khẳng định rằng tám chú ngựa đua thuần chủng tại Trường đua Randwick ở Sydney đã bị nhiễm cúm ngựa.\"\n",
        "\n",
        "encodedInputs = encode_str(inputText, outputText, tokenizer, max_seq_len)\n",
        "\n",
        "outputSent = model.generate(encodedInputs[1].unsqueeze(0), num_beams = 50, num_return_sequences = 3, max_new_tokens = max_seq_len) ##Change this index for vi-lao and lao-vi\n",
        "outputSent = tokenizer.batch_decode(outputSent, skip_special_tokens = True)\n",
        "print(outputSent)"
      ],
      "metadata": {
        "id": "qsiJ4g3-yZ-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2430ece-8792-4607-980d-fb848e638fde"
      },
      "id": "qsiJ4g3-yZ-e",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ມີການຢືນຢັນວ່າ ງົວແປດໂຕ ທີ່ໂຮງຮຽນການແຂ່ງຂັນ ແຣນນວິກ ໃນຊິດນີ ໄດ້ຮັບການຕິດເຊື້ອໄຂ້ຫວັດຫມູຈາກໄຂ້ຫວັດຫມູ.', 'ມີການຢືນຢັນວ່າ ງົວແປດໂຕ ທີ່ໂຮງຮຽນການແຂ່ງຂັນ ລັງເວັກສ ໃນຊິດນີ ໄດ້ຮັບການຕິດເຊື້ອໄຂ້ຫວັດຫມູຈາກໄຂ້ຫວັດຫມູ.', 'ມີການຢືນຢັນວ່າ ງົວແປດໂຕ ທີ່ໂຮງຮຽນການແຂ່ງຂັນ ແຣນນວິດັກ ໃນຊິດນີ ໄດ້ຮັບການຕິດເຊື້ອໄຂ້ຫວັດຫມູຈາກໄຂ້ຫວັດຫມູ.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trueSentenceListLoVi, outputSentenceListLoVi, trueSentenceListViLo, outputSentenceListViLo = eval_model(model, tokenizer, test_dataset)"
      ],
      "metadata": {
        "id": "coizLw3SYOgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894e5693-1650-4d6a-c2a5-c5b77fe16fef"
      },
      "id": "coizLw3SYOgT",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1016\n",
            "1016\n",
            "1016\n",
            "1016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultsLoVi = bleu.compute(predictions = outputSentenceListLoVi, references = trueSentenceListLoVi)\n",
        "print(resultsLoVi)\n",
        "\n",
        "resultsViLo = bleu.compute(predictions = outputSentenceListViLo, references = trueSentenceListViLo)\n",
        "print(resultsViLo)"
      ],
      "metadata": {
        "id": "bmzM-7WXktYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b18188-5761-40fa-b5bf-795b678d4183"
      },
      "id": "bmzM-7WXktYb",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.16683088387053444, 'precisions': [0.44981065803194115, 0.2241573985208604, 0.11944202266782912, 0.06432293226328083], 'brevity_penalty': 1.0, 'length_ratio': 1.0580072000929044, 'translation_length': 36442, 'reference_length': 34444}\n",
            "{'bleu': 0.023738192726294953, 'precisions': [0.1761455525606469, 0.03935185185185185, 0.011550768750487786, 0.003965910049784829], 'brevity_penalty': 1.0, 'length_ratio': 1.4407766990291262, 'translation_length': 14840, 'reference_length': 10300}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/outputSentenceListLoVi.json\", 'w') as f:\n",
        "    json.dump(outputSentenceListLoVi, f, indent = 2, ensure_ascii = False)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/outputSentenceListViLo.json\", 'w') as f:\n",
        "    json.dump(outputSentenceListViLo, f, indent = 2, ensure_ascii = False)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/trueSentenceListLoVi.json\", 'w') as f:\n",
        "    json.dump(trueSentenceListLoVi, f, indent = 2, ensure_ascii = False)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/trueSentenceListViLo.json\", 'w') as f:\n",
        "    json.dump(trueSentenceListViLo, f, indent = 2, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "1rivUs2DyeJJ"
      },
      "id": "1rivUs2DyeJJ",
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}